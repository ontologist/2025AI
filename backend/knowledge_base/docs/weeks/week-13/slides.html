<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Week 13: Reinforcement Learning</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; background: linear-gradient(135deg, #2563eb 0%, #7c3aed 100%); color: #333; overflow: hidden; }
        .slide-container { width: 100vw; height: 100vh; display: flex; align-items: center; justify-content: center; position: relative; }
        .slide { display: none; background: white; width: 90%; max-width: 1200px; height: 85vh; border-radius: 20px; box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3); padding: 60px 80px; position: relative; overflow-y: auto; }
        .slide.active { display: block; animation: slideIn 0.5s ease-out; }
        @keyframes slideIn { from { opacity: 0; transform: translateY(30px); } to { opacity: 1; transform: translateY(0); } }
        .slide h1 { font-size: 3em; color: #2563eb; margin-bottom: 20px; text-align: center; }
        .slide h2 { font-size: 2.2em; color: #7c3aed; margin-bottom: 30px; border-bottom: 3px solid #2563eb; padding-bottom: 10px; }
        .slide h3 { font-size: 1.8em; color: #2563eb; margin-top: 30px; margin-bottom: 15px; }
        .slide h4 { font-size: 1.4em; color: #555; margin-top: 20px; margin-bottom: 10px; }
        .slide p { font-size: 1.2em; line-height: 1.8; margin-bottom: 15px; color: #444; }
        .slide ul, .slide ol { font-size: 1.2em; line-height: 1.8; margin-left: 40px; margin-bottom: 20px; }
        .slide li { margin-bottom: 10px; }
        .bilingual { display: flex; flex-direction: column; gap: 20px; margin: 20px 0; }
        .english, .japanese { padding: 20px; border-radius: 10px; }
        .english { background: #dbeafe; border-left: 4px solid #2563eb; }
        .japanese { background: #f3e8ff; border-left: 4px solid #7c3aed; }
        .highlight-box { background: #fef3cd; border-left: 5px solid #f59e0b; padding: 20px; margin: 20px 0; border-radius: 5px; }
        .code-box { background: #f4f4f4; border: 1px solid #ddd; border-radius: 5px; padding: 15px; font-family: 'Courier New', monospace; margin: 15px 0; overflow-x: auto; white-space: pre-wrap; }
        .navigation { position: fixed; bottom: 30px; left: 50%; transform: translateX(-50%); display: flex; gap: 20px; z-index: 1000; }
        .nav-btn { background: white; color: #2563eb; border: 2px solid #2563eb; padding: 12px 30px; font-size: 1.1em; border-radius: 30px; cursor: pointer; transition: all 0.3s; box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2); }
        .nav-btn:hover { background: #2563eb; color: white; transform: translateY(-2px); box-shadow: 0 6px 20px rgba(0, 0, 0, 0.3); }
        .nav-btn:disabled { opacity: 0.5; cursor: not-allowed; }
        .slide-number { position: fixed; top: 70px; right: 30px; background: rgba(255, 255, 255, 0.9); padding: 10px 20px; border-radius: 20px; font-size: 1.1em; color: #2563eb; font-weight: bold; box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1); z-index: 1000; }
        .title-slide { display: flex; flex-direction: column; justify-content: center; align-items: center; text-align: center; height: 100%; }
        .title-slide h1 { font-size: 4em; margin-bottom: 20px; }
        .title-slide .subtitle { font-size: 2em; color: #7c3aed; margin-bottom: 40px; }
        .title-slide .info { font-size: 1.3em; color: #666; margin: 10px 0; }
        .two-column { display: grid; grid-template-columns: 1fr 1fr; gap: 30px; margin: 20px 0; }
        @media screen and (max-width: 768px) {
            .slide { width: 100vw; height: auto; min-height: 100vh; padding: 20px; border-radius: 0; }
            .bilingual { gap: 15px; }
            .two-column { grid-template-columns: 1fr; gap: 15px; }
        }
    </style>
    <link rel="stylesheet" href="../../styles.css">
</head>
<body>
    <div class="slide-number" id="slideNumber">Slide 1 / 13</div>

    <div class="slide-container">
        <div class="slide active title-slide">
            <h1>Week 13</h1>
            <p class="subtitle">Reinforcement Learning<br>強化学習の基盤としてのn本腕バンディット問題</p>
            <p class="info"><strong>Course:</strong> Introduction to AI and Data Science</p>
            <p class="info"><strong>Topics:</strong> N-Armed Bandit, Exploration vs Exploitation</p>
        </div>

        <div class="slide">
            <h2>Today's Objectives / 本日の目標</h2>
            <div class="bilingual">
                <div class="english">
                    <h3>What You'll Learn:</h3>
                    <ul>
                        <li>Understand reinforcement learning fundamentals</li>
                        <li>Master the n-armed bandit problem</li>
                        <li>Learn exploration vs exploitation tradeoff</li>
                        <li>Explore real-world RL applications</li>
                    </ul>
                </div>
                <div class="japanese">
                    <h3>学ぶこと:</h3>
                    <ul>
                        <li>強化学習の基礎を理解する</li>
                        <li>N本腕バンディット問題を習得する</li>
                        <li>探索と活用のトレードオフを学ぶ</li>
                        <li>実世界のRL応用を探る</li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="slide">
            <h2>What is Reinforcement Learning?</h2>
            <h2>強化学習とは？</h2>
            <div class="highlight-box">
                <h3>Definition / 定義:</h3>
                <p><strong>Learning by interacting with an environment to maximize cumulative reward</strong></p>
                <p><strong>累積報酬を最大化するために環境と相互作用して学習すること</strong></p>
            </div>
            <div class="bilingual">
                <div class="english">
                    <h3>Key Components:</h3>
                    <ul>
                        <li><strong>Agent:</strong> The learner/decision maker</li>
                        <li><strong>Environment:</strong> What the agent interacts with</li>
                        <li><strong>State:</strong> Current situation</li>
                        <li><strong>Action:</strong> What the agent can do</li>
                        <li><strong>Reward:</strong> Feedback signal (positive/negative)</li>
                        <li><strong>Policy:</strong> Strategy for choosing actions</li>
                    </ul>
                </div>
                <div class="japanese">
                    <h3>主要コンポーネント:</h3>
                    <ul>
                        <li><strong>エージェント:</strong> 学習者/意思決定者</li>
                        <li><strong>環境:</strong> エージェントが相互作用するもの</li>
                        <li><strong>状態:</strong> 現在の状況</li>
                        <li><strong>行動:</strong> エージェントができること</li>
                        <li><strong>報酬:</strong> フィードバック信号（正/負）</li>
                        <li><strong>ポリシー:</strong> 行動を選択する戦略</li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="slide">
            <h2>The N-Armed Bandit Problem</h2>
            <h2>N本腕バンディット問題</h2>
            <div class="highlight-box">
                <h3>The Classic Problem / 古典的な問題:</h3>
                <p><strong>You have N slot machines (bandits), each with unknown payout probability</strong></p>
                <p><strong>N台のスロットマシン（バンディット）があり、それぞれ未知の支払い確率を持つ</strong></p>
            </div>
            <div class="code-box">Scenario / シナリオ:

You have 100 plays (limited budget)
100回プレイできる（限られた予算）

Each bandit has different average reward
各バンディットは異なる平均報酬を持つ

Goal: Maximize total reward over 100 plays
目標: 100回プレイで総報酬を最大化

Challenge: You don't know which bandit is best!
課題: どのバンディットが最良かわからない！</div>
        </div>

        <div class="slide">
            <h2>Exploration vs Exploitation</h2>
            <h2>探索 vs 活用</h2>
            <div class="two-column">
                <div>
                    <h3>Exploration / 探索</h3>
                    <div class="highlight-box">
                        <p><strong>Try new actions to discover better options</strong></p>
                        <p><strong>より良い選択肢を発見するために新しい行動を試す</strong></p>
                    </div>
                    <ul>
                        <li>Gather information</li>
                        <li>情報を収集</li>
                        <li>Learn about environment</li>
                        <li>環境について学ぶ</li>
                        <li>Risk: Might get lower rewards</li>
                        <li>リスク: より低い報酬を得る可能性</li>
                    </ul>
                </div>
                <div>
                    <h3>Exploitation / 活用</h3>
                    <div class="highlight-box">
                        <p><strong>Use current knowledge to get maximum reward</strong></p>
                        <p><strong>現在の知識を使って最大の報酬を得る</strong></p>
                    </div>
                    <ul>
                        <li>Choose best known action</li>
                        <li>最良の既知の行動を選択</li>
                        <li>Maximize immediate reward</li>
                        <li>即座の報酬を最大化</li>
                        <li>Risk: Might miss better options</li>
                        <li>リスク: より良い選択肢を見逃す可能性</li>
                    </ul>
                </div>
            </div>
            <div class="highlight-box">
                <p style="text-align: center;"><strong>The Dilemma: Balance exploration to learn vs exploitation to earn!</strong></p>
                <p style="text-align: center;"><strong>ジレンマ: 学ぶための探索と稼ぐための活用のバランス！</strong></p>
            </div>
        </div>

        <div class="slide">
            <h2>Strategies for the Bandit Problem</h2>
            <h2>バンディット問題の戦略</h2>
            <div class="bilingual">
                <div class="english">
                    <h3>Common Approaches:</h3>
                    <ul>
                        <li><strong>Greedy:</strong> Always choose best known option
                            <ul><li>Problem: No exploration, might miss best bandit</li></ul>
                        </li>
                        <li><strong>ε-Greedy (Epsilon-Greedy):</strong>
                            <ul>
                                <li>Explore randomly with probability ε (e.g., 10%)</li>
                                <li>Exploit (choose best) with probability 1-ε (90%)</li>
                                <li>Popular and effective!</li>
                            </ul>
                        </li>
                        <li><strong>Upper Confidence Bound (UCB):</strong>
                            <ul><li>Choose actions based on potential + uncertainty</li></ul>
                        </li>
                    </ul>
                </div>
                <div class="japanese">
                    <h3>一般的なアプローチ:</h3>
                    <ul>
                        <li><strong>貪欲法:</strong> 常に最良の既知のオプションを選択
                            <ul><li>問題: 探索なし、最良のバンディットを見逃す可能性</li></ul>
                        </li>
                        <li><strong>ε-貪欲法（イプシロン貪欲法）:</strong>
                            <ul>
                                <li>確率ε（例: 10%）でランダムに探索</li>
                                <li>確率1-ε（90%）で活用（最良を選択）</li>
                                <li>人気があり効果的！</li>
                            </ul>
                        </li>
                        <li><strong>上側信頼限界（UCB）:</strong>
                            <ul><li>潜在性 + 不確実性に基づいて行動を選択</li></ul>
                        </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="slide">
            <h2>Real-World: A/B Testing</h2>
            <h2>実世界の例: A/Bテスト</h2>
            <div class="highlight-box">
                <p><strong>Application:</strong> Website optimization, ad placement</p>
                <p><strong>Example:</strong> Google Optimize, Optimizely</p>
                <p><strong>Link:</strong> <a href="https://optimize.google.com" target="_blank">Google Optimize</a></p>
            </div>
            <div class="bilingual">
                <div class="english">
                    <h3>Multi-Armed Bandit for A/B Testing:</h3>
                    <ul>
                        <li><strong>Problem:</strong> Which website design converts best?</li>
                        <li><strong>Bandits:</strong> Different webpage versions</li>
                        <li><strong>Reward:</strong> User clicks, purchases, signups</li>
                        <li><strong>Strategy:</strong> ε-greedy or UCB</li>
                        <li><strong>Advantage:</strong> Dynamic allocation - more traffic to better versions</li>
                    </ul>
                    <p><strong>Result:</strong> Maximizes conversions while learning which design is best</p>
                </div>
                <div class="japanese">
                    <h3>A/BテストのためのMulti-Armed Bandit:</h3>
                    <ul>
                        <li><strong>問題:</strong> どのウェブサイトデザインが最もコンバージョンするか？</li>
                        <li><strong>バンディット:</strong> 異なるウェブページバージョン</li>
                        <li><strong>報酬:</strong> ユーザークリック、購入、サインアップ</li>
                        <li><strong>戦略:</strong> ε-貪欲法またはUCB</li>
                        <li><strong>利点:</strong> 動的割り当て - より良いバージョンにより多くのトラフィック</li>
                    </ul>
                    <p><strong>結果:</strong> どのデザインが最良かを学習しながらコンバージョンを最大化</p>
                </div>
            </div>
        </div>

        <div class="slide">
            <h2>Real-World: Content Recommendations</h2>
            <h2>実世界の例: コンテンツ推薦</h2>
            <div class="highlight-box">
                <p><strong>Application:</strong> News articles, videos, music</p>
                <p><strong>Example:</strong> Yahoo News, Medium, TikTok</p>
            </div>
            <div class="bilingual">
                <div class="english">
                    <h3>Contextual Bandits:</h3>
                    <ul>
                        <li><strong>Problem:</strong> Which article to show each user?</li>
                        <li><strong>Context:</strong> User demographics, time, device</li>
                        <li><strong>Actions:</strong> Different articles to recommend</li>
                        <li><strong>Reward:</strong> User clicks, reading time, engagement</li>
                        <li><strong>Learning:</strong> Balance showing popular content vs discovering new interests</li>
                    </ul>
                    <p><strong>Impact:</strong> Increased engagement, personalized experience</p>
                </div>
                <div class="japanese">
                    <h3>コンテキストバンディット:</h3>
                    <ul>
                        <li><strong>問題:</strong> 各ユーザーにどの記事を表示するか？</li>
                        <li><strong>コンテキスト:</strong> ユーザー人口統計、時間、デバイス</li>
                        <li><strong>行動:</strong> 推薦する異なる記事</li>
                        <li><strong>報酬:</strong> ユーザークリック、読書時間、エンゲージメント</li>
                        <li><strong>学習:</strong> 人気コンテンツの表示と新しい興味の発見のバランス</li>
                    </ul>
                    <p><strong>影響:</strong> エンゲージメント増加、パーソナライズされた体験</p>
                </div>
            </div>
        </div>

        <div class="slide">
            <h2>RL Beyond Bandits: Full RL Problems</h2>
            <h2>バンディットを超えたRL: 完全なRL問題</h2>
            <div class="bilingual">
                <div class="english">
                    <h3>From Bandits to Full RL:</h3>
                    <ul>
                        <li><strong>Bandits:</strong> Single state, immediate reward</li>
                        <li><strong>Full RL:</strong> Multiple states, delayed rewards, sequences</li>
                    </ul>
                    <h3>Full RL Components:</h3>
                    <ul>
                        <li><strong>States:</strong> Different situations agent can be in</li>
                        <li><strong>Transitions:</strong> How actions move between states</li>
                        <li><strong>Long-term reward:</strong> Consider future consequences</li>
                        <li><strong>Policy:</strong> Complete strategy for all states</li>
                    </ul>
                </div>
                <div class="japanese">
                    <h3>バンディットから完全なRLへ:</h3>
                    <ul>
                        <li><strong>バンディット:</strong> 単一状態、即座の報酬</li>
                        <li><strong>完全なRL:</strong> 複数の状態、遅延報酬、シーケンス</li>
                    </ul>
                    <h3>完全なRLコンポーネント:</h3>
                    <ul>
                        <li><strong>状態:</strong> エージェントが取り得る異なる状況</li>
                        <li><strong>遷移:</strong> 行動が状態間でどう移動するか</li>
                        <li><strong>長期報酬:</strong> 将来の結果を考慮</li>
                        <li><strong>ポリシー:</strong> すべての状態に対する完全な戦略</li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="slide">
            <h2>Real-World: Game AI - AlphaGo (Revisited)</h2>
            <h2>実世界の例: ゲームAI - AlphaGo（再訪）</h2>
            <div class="highlight-box">
                <p><strong>Link:</strong> <a href="https://deepmind.google/technologies/alphago/" target="_blank">DeepMind AlphaGo</a></p>
            </div>
            <div class="bilingual">
                <div class="english">
                    <h3>Full RL in Action:</h3>
                    <ul>
                        <li><strong>States:</strong> All possible board configurations</li>
                        <li><strong>Actions:</strong> Placing a stone on the board</li>
                        <li><strong>Reward:</strong> Win (+1), Loss (-1), Draw (0)</li>
                        <li><strong>Challenge:</strong> Reward only at end of game (delayed)</li>
                        <li><strong>Training:</strong> Self-play millions of games</li>
                        <li><strong>Algorithm:</strong> Monte Carlo Tree Search + Deep RL</li>
                    </ul>
                    <p><strong>Achievement:</strong> Defeated world champion 2016</p>
                </div>
                <div class="japanese">
                    <h3>実際の完全なRL:</h3>
                    <ul>
                        <li><strong>状態:</strong> すべての可能な盤面配置</li>
                        <li><strong>行動:</strong> 盤上に石を置く</li>
                        <li><strong>報酬:</strong> 勝ち（+1）、負け（-1）、引き分け（0）</li>
                        <li><strong>課題:</strong> ゲーム終了時のみ報酬（遅延）</li>
                        <li><strong>訓練:</strong> 数百万ゲームの自己対局</li>
                        <li><strong>アルゴリズム:</strong> モンテカルロ木探索 + 深層RL</li>
                    </ul>
                    <p><strong>成果:</strong> 2016年に世界チャンピオンを破る</p>
                </div>
            </div>
        </div>

        <div class="slide">
            <h2>Real-World: Robotics</h2>
            <h2>実世界の例: ロボティクス</h2>
            <div class="highlight-box">
                <p><strong>Application:</strong> Industrial robots, autonomous systems</p>
                <p><strong>Example:</strong> Boston Dynamics robots</p>
            </div>
            <div class="bilingual">
                <div class="english">
                    <h3>RL for Robot Control:</h3>
                    <ul>
                        <li><strong>Task:</strong> Walking, grasping, navigation</li>
                        <li><strong>States:</strong> Joint angles, positions, velocities</li>
                        <li><strong>Actions:</strong> Motor commands</li>
                        <li><strong>Reward:</strong> Progress toward goal, stability</li>
                        <li><strong>Penalty:</strong> Falling, collisions</li>
                        <li><strong>Learning:</strong> Trial and error in simulation, then real world</li>
                    </ul>
                    <p><strong>Advantage:</strong> Learns complex movements without explicit programming</p>
                </div>
                <div class="japanese">
                    <h3>ロボット制御のためのRL:</h3>
                    <ul>
                        <li><strong>タスク:</strong> 歩行、把持、ナビゲーション</li>
                        <li><strong>状態:</strong> 関節角度、位置、速度</li>
                        <li><strong>行動:</strong> モーターコマンド</li>
                        <li><strong>報酬:</strong> 目標への進行、安定性</li>
                        <li><strong>ペナルティ:</strong> 転倒、衝突</li>
                        <li><strong>学習:</strong> シミュレーションでの試行錯誤、その後実世界</li>
                    </ul>
                    <p><strong>利点:</strong> 明示的なプログラミングなしで複雑な動きを学習</p>
                </div>
            </div>
        </div>

        <div class="slide">
            <h2>RL Algorithms Overview</h2>
            <h2>RLアルゴリズム概要</h2>
            <div class="two-column">
                <div>
                    <h3>Value-Based Methods</h3>
                    <h4>価値ベース手法</h4>
                    <ul>
                        <li><strong>Q-Learning:</strong> Learn value of state-action pairs</li>
                        <li><strong>Deep Q-Network (DQN):</strong> Q-learning with neural networks</li>
                        <li><strong>Goal:</strong> Estimate which actions are most valuable</li>
                    </ul>
                </div>
                <div>
                    <h3>Policy-Based Methods</h3>
                    <h4>ポリシーベース手法</h4>
                    <ul>
                        <li><strong>Policy Gradient:</strong> Directly learn policy</li>
                        <li><strong>Actor-Critic:</strong> Combine value and policy</li>
                        <li><strong>Goal:</strong> Learn optimal strategy directly</li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="slide">
            <h2>Practical Considerations</h2>
            <h2>実践的な考慮事項</h2>
            <div class="bilingual">
                <div class="english">
                    <h3>Challenges in RL:</h3>
                    <ul>
                        <li><strong>Sample Efficiency:</strong> Needs many interactions to learn</li>
                        <li><strong>Credit Assignment:</strong> Which action caused the reward?</li>
                        <li><strong>Exploration:</strong> How to explore safely and efficiently?</li>
                        <li><strong>Stability:</strong> RL training can be unstable</li>
                    </ul>
                    <h3>When to Use RL:</h3>
                    <ul>
                        <li>Sequential decision making</li>
                        <li>Can simulate environment</li>
                        <li>Clear reward signal</li>
                        <li>Trial and error is acceptable</li>
                    </ul>
                </div>
                <div class="japanese">
                    <h3>RLにおける課題:</h3>
                    <ul>
                        <li><strong>サンプル効率:</strong> 学習に多くの相互作用が必要</li>
                        <li><strong>クレジット割り当て:</strong> どの行動が報酬を引き起こしたか？</li>
                        <li><strong>探索:</strong> 安全かつ効率的に探索する方法は？</li>
                        <li><strong>安定性:</strong> RL訓練は不安定になる可能性</li>
                    </ul>
                    <h3>RLを使用する場合:</h3>
                    <ul>
                        <li>逐次的意思決定</li>
                        <li>環境をシミュレートできる</li>
                        <li>明確な報酬信号</li>
                        <li>試行錯誤が許容される</li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="slide">
            <h2>Summary / まとめ</h2>
            <div class="bilingual">
                <div class="english">
                    <h3>Key Takeaways:</h3>
                    <ul>
                        <li>RL: Learning through interaction to maximize reward</li>
                        <li>N-Armed Bandit: Foundation RL problem, single state</li>
                        <li>Exploration vs Exploitation: Key tradeoff in RL</li>
                        <li>ε-Greedy: Simple effective strategy for bandits</li>
                        <li>Real applications: A/B testing, recommendations, game AI, robotics</li>
                        <li>Full RL: Multiple states, delayed rewards, complex policies</li>
                    </ul>
                </div>
                <div class="japanese">
                    <h3>重要ポイント:</h3>
                    <ul>
                        <li>RL: 報酬を最大化するために相互作用を通じて学習</li>
                        <li>N本腕バンディット: 基礎的なRL問題、単一状態</li>
                        <li>探索 vs 活用: RLにおける重要なトレードオフ</li>
                        <li>ε-貪欲法: バンディットのためのシンプルで効果的な戦略</li>
                        <li>実際の応用: A/Bテスト、推薦、ゲームAI、ロボティクス</li>
                        <li>完全なRL: 複数の状態、遅延報酬、複雑なポリシー</li>
                    </ul>
                </div>
            </div>
            <h3 style="text-align: center; margin-top: 40px;">Next Week / 来週:</h3>
            <p style="text-align: center; font-size: 1.3em;">In-Class Examination Review - Comprehensive Course Review</p>
            <p style="text-align: center; font-size: 1.3em;">授業中試験 - 総合コース復習</p>
        </div>

    </div>

    <div class="navigation">
        <button class="nav-btn" id="prevBtn" onclick="changeSlide(-1)">◀ Previous</button>
        <button class="nav-btn" id="nextBtn" onclick="changeSlide(1)">Next ▶</button>
    </div>

    <script>
        let currentSlide = 0;
        const slides = document.querySelectorAll('.slide');
        const totalSlides = slides.length;

        function showSlide(n) {
            slides[currentSlide].classList.remove('active');
            currentSlide = (n + totalSlides) % totalSlides;
            slides[currentSlide].classList.add('active');

            document.getElementById('slideNumber').textContent = `Slide ${currentSlide + 1} / ${totalSlides}`;

            document.getElementById('prevBtn').disabled = currentSlide === 0;
            document.getElementById('nextBtn').disabled = currentSlide === totalSlides - 1;
        }

        function changeSlide(direction) {
            showSlide(currentSlide + direction);
        }

        document.addEventListener('keydown', function(event) {
            if (event.key === 'ArrowLeft') {
                changeSlide(-1);
            } else if (event.key === 'ArrowRight' || event.key === ' ') {
                event.preventDefault();
                changeSlide(1);
            } else if (event.key === 'Home') {
                showSlide(0);
            } else if (event.key === 'End') {
                showSlide(totalSlides - 1);
            }
        });

        showSlide(0);
    </script>
</body>
</html>
