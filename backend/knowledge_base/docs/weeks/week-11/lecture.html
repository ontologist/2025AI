<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Week 11 Lecture Notes: Classification Algorithms</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
            line-height: 1.6;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 40px;
            border-radius: 15px;
            box-shadow: 0 10px 40px rgba(0,0,0,0.2);
        }

        h1 {
            color: #2563eb;
            font-size: 2.5em;
            margin-bottom: 10px;
            text-align: center;
            border-bottom: 4px solid #7c3aed;
            padding-bottom: 20px;
        }

        h2 {
            color: #7c3aed;
            font-size: 2em;
            margin-top: 40px;
            margin-bottom: 20px;
            border-left: 6px solid #2563eb;
            padding-left: 15px;
        }

        h3 {
            color: #2563eb;
            font-size: 1.5em;
            margin-top: 30px;
            margin-bottom: 15px;
        }

        h4 {
            color: #555;
            font-size: 1.2em;
            margin-top: 20px;
            margin-bottom: 10px;
        }

        .subtitle {
            text-align: center;
            color: #666;
            font-size: 1.2em;
            margin-bottom: 30px;
        }

        .bilingual {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
            margin: 20px 0;
        }

        .english {
            background: #dbeafe;
            padding: 20px;
            border-radius: 10px;
            border-left: 4px solid #2563eb;
        }

        .japanese {
            background: #f3e8ff;
            padding: 20px;
            border-radius: 10px;
            border-left: 4px solid #7c3aed;
        }

        .highlight-box {
            background: #fef3cd;
            border-left: 5px solid #f59e0b;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
        }

        .definition-box {
            background: #e0f2fe;
            border: 2px solid #0ea5e9;
            padding: 20px;
            margin: 20px 0;
            border-radius: 10px;
        }

        .code-box {
            background: #f4f4f4;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 20px;
            font-family: 'Courier New', monospace;
            margin: 20px 0;
            white-space: pre-wrap;
            overflow-x: auto;
        }

        .test-question {
            background: #fce7f3;
            border: 3px solid #ec4899;
            padding: 25px;
            margin: 25px 0;
            border-radius: 10px;
        }

        .test-question h4::before {
            content: "âš ï¸ ";
        }

        ul, ol {
            margin-left: 40px;
            margin-bottom: 20px;
        }

        li {
            margin-bottom: 10px;
        }

        p {
            margin-bottom: 15px;
        }

        .navigation {
            background: #e0e7ff;
            padding: 20px;
            border-radius: 10px;
            margin: 30px 0;
            text-align: center;
        }

        .navigation a {
            display: inline-block;
            margin: 10px;
            padding: 12px 24px;
            background: #2563eb;
            color: white;
            text-decoration: none;
            border-radius: 5px;
            transition: all 0.3s;
        }

        .navigation a:hover {
            background: #7c3aed;
            transform: translateY(-2px);
        }

        .key-terms {
            background: #f0fdf4;
            border-left: 5px solid #10b981;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
        }

        .study-tip {
            background: #fff7ed;
            border-left: 5px solid #f97316;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
        }

        @media (max-width: 768px) {
            .bilingual {
                grid-template-columns: 1fr;
            }

            .container {
                padding: 20px;
            }

            h1 {
                font-size: 2em;
            }

            h2 {
                font-size: 1.6em;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Week 11 Lecture Notes</h1>
        <p class="subtitle">Classification Algorithms<br>åˆ†é¡ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </p>

        <div class="navigation">
            <a href="../../index.html">â† Course Home</a>
            <a href="../week-10/lecture.html">â† Previous Week</a>
            <a href="slides.html">View Slides</a>
            <a href="assignment.html">Assignment</a>
        </div>

        <h2>ğŸ“š Overview / æ¦‚è¦</h2>
        <div class="bilingual">
            <div class="english">
                <p>This week explores advanced classification algorithms beyond logistic regression. We'll study Decision Trees, Random Forests, Support Vector Machines (SVM), and k-Nearest Neighbors (k-NN). Each algorithm has unique strengths and is suited for different types of classification problems.</p>
                <p><strong>Learning Objectives:</strong></p>
                <ul>
                    <li>Understand how Decision Trees make classifications</li>
                    <li>Learn about ensemble methods like Random Forests</li>
                    <li>Master the k-Nearest Neighbors algorithm</li>
                    <li>Explore Support Vector Machines and kernel tricks</li>
                    <li>Compare different classification algorithms</li>
                    <li>Apply appropriate algorithms to real-world problems</li>
                </ul>
            </div>
            <div class="japanese">
                <p>ä»Šé€±ã¯ã€ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã‚’è¶…ãˆãŸé«˜åº¦ãªåˆ†é¡ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æ¢ã‚Šã¾ã™ã€‚æ±ºå®šæœ¨ã€ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã€ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚·ãƒ³ï¼ˆSVMï¼‰ã€kè¿‘å‚æ³•ï¼ˆk-NNï¼‰ã‚’å­¦ã³ã¾ã™ã€‚å„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã¯ç‹¬è‡ªã®å¼·ã¿ãŒã‚ã‚Šã€ç•°ãªã‚‹ã‚¿ã‚¤ãƒ—ã®åˆ†é¡å•é¡Œã«é©ã—ã¦ã„ã¾ã™ã€‚</p>
                <p><strong>å­¦ç¿’ç›®æ¨™:</strong></p>
                <ul>
                    <li>æ±ºå®šæœ¨ãŒã©ã®ã‚ˆã†ã«åˆ†é¡ã‚’è¡Œã†ã‹ã‚’ç†è§£ã™ã‚‹</li>
                    <li>ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®ã‚ˆã†ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã«ã¤ã„ã¦å­¦ã¶</li>
                    <li>kè¿‘å‚æ³•ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç¿’å¾—ã™ã‚‹</li>
                    <li>ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚·ãƒ³ã¨ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯ã‚’æ¢ã‚‹</li>
                    <li>ç•°ãªã‚‹åˆ†é¡ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æ¯”è¼ƒã™ã‚‹</li>
                    <li>å®Ÿä¸–ç•Œã®å•é¡Œã«é©åˆ‡ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é©ç”¨ã™ã‚‹</li>
                </ul>
            </div>
        </div>

        <h2>1. Decision Trees / æ±ºå®šæœ¨</h2>

        <div class="definition-box">
            <h3>What is a Decision Tree? / æ±ºå®šæœ¨ã¨ã¯ï¼Ÿ</h3>
            <p><strong>Decision Tree:</strong> A supervised learning algorithm that makes decisions by learning simple decision rules from data features, creating a tree-like model of decisions and their possible consequences.</p>
            <p><strong>æ±ºå®šæœ¨:</strong> ãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´é‡ã‹ã‚‰å˜ç´”ãªæ±ºå®šãƒ«ãƒ¼ãƒ«ã‚’å­¦ç¿’ã—ã€æ±ºå®šã¨ãã®å¯èƒ½æ€§ã®ã‚ã‚‹çµæœã®ãƒ„ãƒªãƒ¼çŠ¶ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã™ã‚‹ã“ã¨ã§æ±ºå®šã‚’è¡Œã†æ•™å¸«ã‚ã‚Šå­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€‚</p>
        </div>

        <div class="bilingual">
            <div class="english">
                <h3>How Decision Trees Work</h3>

                <h4>Tree Structure</h4>
                <div class="code-box">Example: Will customer buy product?

                   [Income > $50k?]  â† Root Node (æœ€åˆã®è³ªå•)
                    /            \
                 Yes              No
                  /                \
         [Age > 30?]         [Has kids?]  â† Internal Nodes
          /      \            /      \
       Yes       No         Yes      No
        /         \          /        \
     BUY      MAYBE      BUY      DON'T BUY  â† Leaf Nodes (äºˆæ¸¬)
</div>

                <h4>Key Components:</h4>
                <ul>
                    <li><strong>Root Node:</strong> The first decision, represents the best feature to split on</li>
                    <li><strong>Internal Nodes:</strong> Each represents a decision based on a feature</li>
                    <li><strong>Branches:</strong> Represent the outcome of a decision</li>
                    <li><strong>Leaf Nodes:</strong> Final predictions (class labels)</li>
                </ul>

                <h4>How Trees Learn (Building the Tree)</h4>
                <ol>
                    <li><strong>Select Best Feature:</strong> Choose feature that best separates classes
                        <ul>
                            <li>Uses metrics like Information Gain or Gini Impurity</li>
                            <li>Goal: Maximize purity of resulting groups</li>
                        </ul>
                    </li>
                    <li><strong>Split Data:</strong> Divide data based on feature value</li>
                    <li><strong>Repeat Recursively:</strong> For each branch, repeat steps 1-2</li>
                    <li><strong>Stop When:</strong>
                        <ul>
                            <li>All samples in node belong to same class</li>
                            <li>Maximum depth reached</li>
                            <li>Minimum samples per node reached</li>
                        </ul>
                    </li>
                </ol>

                <h4>Advantages:</h4>
                <ul>
                    <li><strong>Interpretable:</strong> Easy to understand and visualize</li>
                    <li><strong>No scaling needed:</strong> Works with features of different scales</li>
                    <li><strong>Handles mixed data:</strong> Both numerical and categorical</li>
                    <li><strong>Non-linear:</strong> Can capture complex patterns</li>
                    <li><strong>Feature importance:</strong> Shows which features matter most</li>
                </ul>

                <h4>Disadvantages:</h4>
                <ul>
                    <li><strong>Overfitting:</strong> Can create overly complex trees</li>
                    <li><strong>Unstable:</strong> Small changes in data can change tree structure</li>
                    <li><strong>Biased:</strong> Favors features with more levels</li>
                    <li><strong>Not optimal:</strong> Greedy algorithm may miss best solution</li>
                </ul>
            </div>
            <div class="japanese">
                <h3>æ±ºå®šæœ¨ã®ä»•çµ„ã¿</h3>

                <h4>ãƒ„ãƒªãƒ¼æ§‹é€ </h4>
                <div class="code-box">ä¾‹: é¡§å®¢ã¯è£½å“ã‚’è³¼å…¥ã™ã‚‹ã‹ï¼Ÿ

                [åå…¥ > $50k?]  â† ãƒ«ãƒ¼ãƒˆãƒãƒ¼ãƒ‰ï¼ˆæœ€åˆã®è³ªå•ï¼‰
                /            \
             ã¯ã„              ã„ã„ãˆ
              /                \
       [å¹´é½¢ > 30?]         [å­ä¾›ã‚ã‚Š?]  â† å†…éƒ¨ãƒãƒ¼ãƒ‰
        /      \            /      \
     ã¯ã„      ã„ã„ãˆ      ã¯ã„      ã„ã„ãˆ
      /         \          /        \
   è³¼å…¥      å¤šåˆ†      è³¼å…¥     è³¼å…¥ã—ãªã„  â† ãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰ï¼ˆäºˆæ¸¬ï¼‰
</div>

                <h4>ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ:</h4>
                <ul>
                    <li><strong>ãƒ«ãƒ¼ãƒˆãƒãƒ¼ãƒ‰:</strong> æœ€åˆã®æ±ºå®šã€åˆ†å‰²ã™ã‚‹æœ€é©ãªç‰¹å¾´é‡ã‚’è¡¨ã™</li>
                    <li><strong>å†…éƒ¨ãƒãƒ¼ãƒ‰:</strong> ãã‚Œãã‚Œç‰¹å¾´é‡ã«åŸºã¥ãæ±ºå®šã‚’è¡¨ã™</li>
                    <li><strong>ãƒ–ãƒ©ãƒ³ãƒ:</strong> æ±ºå®šã®çµæœã‚’è¡¨ã™</li>
                    <li><strong>ãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰:</strong> æœ€çµ‚äºˆæ¸¬ï¼ˆã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ï¼‰</li>
                </ul>

                <h4>ãƒ„ãƒªãƒ¼ã®å­¦ç¿’æ–¹æ³•ï¼ˆãƒ„ãƒªãƒ¼ã®æ§‹ç¯‰ï¼‰</h4>
                <ol>
                    <li><strong>æœ€é©ãªç‰¹å¾´é‡ã‚’é¸æŠ:</strong> ã‚¯ãƒ©ã‚¹ã‚’æœ€ã‚‚ã‚ˆãåˆ†é›¢ã™ã‚‹ç‰¹å¾´é‡ã‚’é¸æŠ
                        <ul>
                            <li>æƒ…å ±åˆ©å¾—ã‚„ã‚¸ãƒ‹ä¸ç´”åº¦ãªã©ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä½¿ç”¨</li>
                            <li>ç›®æ¨™: çµæœã‚°ãƒ«ãƒ¼ãƒ—ã®ç´”åº¦ã‚’æœ€å¤§åŒ–</li>
                        </ul>
                    </li>
                    <li><strong>ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²:</strong> ç‰¹å¾´é‡ã®å€¤ã«åŸºã¥ã„ã¦ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²</li>
                    <li><strong>å†å¸°çš„ã«ç¹°ã‚Šè¿”ã—:</strong> å„ãƒ–ãƒ©ãƒ³ãƒã«å¯¾ã—ã¦ã€ã‚¹ãƒ†ãƒƒãƒ—1-2ã‚’ç¹°ã‚Šè¿”ã™</li>
                    <li><strong>åœæ­¢æ¡ä»¶:</strong>
                        <ul>
                            <li>ãƒãƒ¼ãƒ‰å†…ã®ã™ã¹ã¦ã®ã‚µãƒ³ãƒ—ãƒ«ãŒåŒã˜ã‚¯ãƒ©ã‚¹ã«å±ã™ã‚‹</li>
                            <li>æœ€å¤§æ·±åº¦ã«åˆ°é”</li>
                            <li>ãƒãƒ¼ãƒ‰ã”ã¨ã®æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°ã«åˆ°é”</li>
                        </ul>
                    </li>
                </ol>

                <h4>åˆ©ç‚¹:</h4>
                <ul>
                    <li><strong>è§£é‡ˆå¯èƒ½:</strong> ç†è§£ã¨å¯è¦–åŒ–ãŒå®¹æ˜“</li>
                    <li><strong>ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¸è¦:</strong> ç•°ãªã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ã®ç‰¹å¾´é‡ã§æ©Ÿèƒ½</li>
                    <li><strong>æ··åˆãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†:</strong> æ•°å€¤ã¨ã‚«ãƒ†ã‚´ãƒªä¸¡æ–¹</li>
                    <li><strong>éç·šå½¢:</strong> è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ‰ãˆã‚‰ã‚Œã‚‹</li>
                    <li><strong>ç‰¹å¾´é‡ã®é‡è¦åº¦:</strong> ã©ã®ç‰¹å¾´é‡ãŒæœ€ã‚‚é‡è¦ã‹ã‚’ç¤ºã™</li>
                </ul>

                <h4>æ¬ ç‚¹:</h4>
                <ul>
                    <li><strong>éå­¦ç¿’:</strong> éåº¦ã«è¤‡é›‘ãªãƒ„ãƒªãƒ¼ã‚’ä½œæˆã™ã‚‹å¯èƒ½æ€§</li>
                    <li><strong>ä¸å®‰å®š:</strong> ãƒ‡ãƒ¼ã‚¿ã®å°ã•ãªå¤‰åŒ–ã§ãƒ„ãƒªãƒ¼æ§‹é€ ãŒå¤‰ã‚ã‚‹å¯èƒ½æ€§</li>
                    <li><strong>åã‚Š:</strong> ã‚ˆã‚Šå¤šãã®ãƒ¬ãƒ™ãƒ«ã‚’æŒã¤ç‰¹å¾´é‡ã‚’å„ªé‡</li>
                    <li><strong>æœ€é©ã§ãªã„:</strong> è²ªæ¬²ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯æœ€è‰¯ã®è§£ã‚’è¦‹é€ƒã™å¯èƒ½æ€§</li>
                </ul>
            </div>
        </div>

        <h2>2. Random Forests / ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ</h2>

        <div class="definition-box">
            <h3>What is a Random Forest? / ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã¨ã¯ï¼Ÿ</h3>
            <p><strong>Random Forest:</strong> An ensemble learning method that creates multiple decision trees and combines their predictions through voting (classification) or averaging (regression) to improve accuracy and reduce overfitting.</p>
            <p><strong>ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ:</strong> è¤‡æ•°ã®æ±ºå®šæœ¨ã‚’ä½œæˆã—ã€æŠ•ç¥¨ï¼ˆåˆ†é¡ï¼‰ã¾ãŸã¯å¹³å‡åŒ–ï¼ˆå›å¸°ï¼‰ã‚’é€šã˜ã¦ãã‚Œã‚‰ã®äºˆæ¸¬ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ç²¾åº¦ã‚’å‘ä¸Šã•ã›éå­¦ç¿’ã‚’æ¸›ã‚‰ã™ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’æ‰‹æ³•ã€‚</p>
        </div>

        <div class="bilingual">
            <div class="english">
                <h3>The Wisdom of Crowds</h3>

                <h4>How Random Forest Works</h4>
                <div class="code-box">RANDOM FOREST PROCESS

Step 1: Bootstrap Sampling (ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°)
â”œâ”€ Create N different random samples from training data
â”œâ”€ Each sample: randomly select data with replacement
â””â”€ Different trees see different data

Step 2: Build Multiple Trees (è¤‡æ•°ã®æœ¨ã‚’æ§‹ç¯‰)
â”œâ”€ Train N decision trees (e.g., 100 trees)
â”œâ”€ Each tree built on different bootstrap sample
â””â”€ Random feature selection at each split

Step 3: Make Predictions (äºˆæ¸¬ã‚’è¡Œã†)
â”œâ”€ Each tree makes independent prediction
â””â”€ Classification: Majority vote wins
    Regression: Average all predictions

Example with 100 trees predicting spam:
Tree 1: Spam     Tree 2: Spam     Tree 3: Not Spam
Tree 4: Spam     Tree 5: Spam     ...
â””â”€ 73 trees say "Spam", 27 say "Not Spam"
   â†’ Final Prediction: Spam (majority vote)</div>

                <h4>Key Concepts:</h4>
                <ul>
                    <li><strong>Bootstrap Sampling:</strong> Randomly sample data with replacement
                        <ul>
                            <li>Creates diverse training sets</li>
                            <li>Each tree sees different examples</li>
                        </ul>
                    </li>
                    <li><strong>Feature Randomness:</strong> At each split, consider only random subset of features
                        <ul>
                            <li>Prevents correlation between trees</li>
                            <li>Increases diversity</li>
                        </ul>
                    </li>
                    <li><strong>Ensemble Voting:</strong> Combine predictions from all trees
                        <ul>
                            <li>Reduces impact of individual errors</li>
                            <li>More robust than single tree</li>
                        </ul>
                    </li>
                </ul>

                <h4>Why Random Forests Work Better</h4>
                <ul>
                    <li><strong>Reduces Overfitting:</strong> Individual trees may overfit, but averaging cancels out errors</li>
                    <li><strong>Handles Missing Data:</strong> Can maintain accuracy even with missing values</li>
                    <li><strong>Feature Importance:</strong> Ranks which features are most useful</li>
                    <li><strong>Works Well Out-of-Box:</strong> Often good with default parameters</li>
                    <li><strong>Parallelizable:</strong> Trees can be built independently</li>
                </ul>

                <h4>Disadvantages:</h4>
                <ul>
                    <li><strong>Less Interpretable:</strong> Hard to understand 100+ trees</li>
                    <li><strong>Slower:</strong> Training many trees takes time</li>
                    <li><strong>Memory:</strong> Requires storing multiple trees</li>
                    <li><strong>Not for Real-time:</strong> Slower prediction than single tree</li>
                </ul>
            </div>
            <div class="japanese">
                <h3>ç¾¤è¡†ã®çŸ¥æµ</h3>

                <h4>ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã®ä»•çµ„ã¿</h4>
                <div class="code-box">ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãƒ—ãƒ­ã‚»ã‚¹

ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
â”œâ”€ è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰Nå€‹ã®ç•°ãªã‚‹ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½œæˆ
â”œâ”€ å„ã‚µãƒ³ãƒ—ãƒ«: ç½®æ›ã‚ã‚Šã§ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ
â””â”€ ç•°ãªã‚‹ãƒ„ãƒªãƒ¼ãŒç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ã‚‹

ã‚¹ãƒ†ãƒƒãƒ—2: è¤‡æ•°ã®ãƒ„ãƒªãƒ¼ã‚’æ§‹ç¯‰
â”œâ”€ Nå€‹ã®æ±ºå®šæœ¨ã‚’è¨“ç·´ï¼ˆä¾‹: 100æœ¬ã®ãƒ„ãƒªãƒ¼ï¼‰
â”œâ”€ å„ãƒ„ãƒªãƒ¼ã¯ç•°ãªã‚‹ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒ«ã§æ§‹ç¯‰
â””â”€ å„åˆ†å‰²ã§ãƒ©ãƒ³ãƒ€ãƒ ãªç‰¹å¾´é‡é¸æŠ

ã‚¹ãƒ†ãƒƒãƒ—3: äºˆæ¸¬ã‚’è¡Œã†
â”œâ”€ å„ãƒ„ãƒªãƒ¼ãŒç‹¬ç«‹ã—ãŸäºˆæ¸¬ã‚’è¡Œã†
â””â”€ åˆ†é¡: å¤šæ•°æ±ºãŒå‹ã¤
    å›å¸°: ã™ã¹ã¦ã®äºˆæ¸¬ã‚’å¹³å‡

ã‚¹ãƒ‘ãƒ äºˆæ¸¬ã™ã‚‹100æœ¬ã®ãƒ„ãƒªãƒ¼ã®ä¾‹:
ãƒ„ãƒªãƒ¼1: ã‚¹ãƒ‘ãƒ    ãƒ„ãƒªãƒ¼2: ã‚¹ãƒ‘ãƒ    ãƒ„ãƒªãƒ¼3: éã‚¹ãƒ‘ãƒ 
ãƒ„ãƒªãƒ¼4: ã‚¹ãƒ‘ãƒ    ãƒ„ãƒªãƒ¼5: ã‚¹ãƒ‘ãƒ    ...
â””â”€ 73æœ¬ãŒã€Œã‚¹ãƒ‘ãƒ ã€ã€27æœ¬ãŒã€Œéã‚¹ãƒ‘ãƒ ã€
   â†’ æœ€çµ‚äºˆæ¸¬: ã‚¹ãƒ‘ãƒ ï¼ˆå¤šæ•°æ±ºï¼‰</div>

                <h4>ä¸»è¦æ¦‚å¿µ:</h4>
                <ul>
                    <li><strong>ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°:</strong> ç½®æ›ã‚ã‚Šã§ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒ«
                        <ul>
                            <li>å¤šæ§˜ãªè¨“ç·´ã‚»ãƒƒãƒˆã‚’ä½œæˆ</li>
                            <li>å„ãƒ„ãƒªãƒ¼ãŒç•°ãªã‚‹ä¾‹ã‚’è¦‹ã‚‹</li>
                        </ul>
                    </li>
                    <li><strong>ç‰¹å¾´é‡ã®ãƒ©ãƒ³ãƒ€ãƒ æ€§:</strong> å„åˆ†å‰²ã§ã€ç‰¹å¾´é‡ã®ãƒ©ãƒ³ãƒ€ãƒ ãªã‚µãƒ–ã‚»ãƒƒãƒˆã®ã¿ã‚’è€ƒæ…®
                        <ul>
                            <li>ãƒ„ãƒªãƒ¼é–“ã®ç›¸é–¢ã‚’é˜²ã</li>
                            <li>å¤šæ§˜æ€§ã‚’å¢—ã‚„ã™</li>
                        </ul>
                    </li>
                    <li><strong>ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æŠ•ç¥¨:</strong> ã™ã¹ã¦ã®ãƒ„ãƒªãƒ¼ã‹ã‚‰ã®äºˆæ¸¬ã‚’çµ„ã¿åˆã‚ã›ã‚‹
                        <ul>
                            <li>å€‹ã€…ã®ã‚¨ãƒ©ãƒ¼ã®å½±éŸ¿ã‚’æ¸›ã‚‰ã™</li>
                            <li>å˜ä¸€ã®ãƒ„ãƒªãƒ¼ã‚ˆã‚Šã‚‚å …ç‰¢</li>
                        </ul>
                    </li>
                </ul>

                <h4>ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆãŒã‚ˆã‚Šè‰¯ãæ©Ÿèƒ½ã™ã‚‹ç†ç”±</h4>
                <ul>
                    <li><strong>éå­¦ç¿’ã‚’æ¸›ã‚‰ã™:</strong> å€‹ã€…ã®ãƒ„ãƒªãƒ¼ã¯éå­¦ç¿’ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŒã€å¹³å‡åŒ–ã«ã‚ˆã‚Šã‚¨ãƒ©ãƒ¼ãŒç›¸æ®ºã•ã‚Œã‚‹</li>
                    <li><strong>æ¬ æãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†:</strong> æ¬ æå€¤ãŒã‚ã£ã¦ã‚‚ç²¾åº¦ã‚’ç¶­æŒã§ãã‚‹</li>
                    <li><strong>ç‰¹å¾´é‡ã®é‡è¦åº¦:</strong> ã©ã®ç‰¹å¾´é‡ãŒæœ€ã‚‚æœ‰ç”¨ã‹ã‚’ãƒ©ãƒ³ã‚¯ä»˜ã‘</li>
                    <li><strong>ã™ãã«ä½¿ãˆã‚‹:</strong> ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§è‰¯å¥½ãªã“ã¨ãŒå¤šã„</li>
                    <li><strong>ä¸¦åˆ—åŒ–å¯èƒ½:</strong> ãƒ„ãƒªãƒ¼ã‚’ç‹¬ç«‹ã—ã¦æ§‹ç¯‰ã§ãã‚‹</li>
                </ul>

                <h4>æ¬ ç‚¹:</h4>
                <ul>
                    <li><strong>è§£é‡ˆæ€§ãŒä½ã„:</strong> 100ä»¥ä¸Šã®ãƒ„ãƒªãƒ¼ã‚’ç†è§£ã™ã‚‹ã®ã¯å›°é›£</li>
                    <li><strong>é…ã„:</strong> å¤šãã®ãƒ„ãƒªãƒ¼ã®è¨“ç·´ã«æ™‚é–“ãŒã‹ã‹ã‚‹</li>
                    <li><strong>ãƒ¡ãƒ¢ãƒª:</strong> è¤‡æ•°ã®ãƒ„ãƒªãƒ¼ã‚’ä¿å­˜ã™ã‚‹å¿…è¦ãŒã‚ã‚‹</li>
                    <li><strong>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‘ãã§ãªã„:</strong> å˜ä¸€ãƒ„ãƒªãƒ¼ã‚ˆã‚Šã‚‚äºˆæ¸¬ãŒé…ã„</li>
                </ul>
            </div>
        </div>

        <h2>3. k-Nearest Neighbors (k-NN) / kè¿‘å‚æ³•</h2>

        <div class="definition-box">
            <h3>What is k-NN? / k-NNã¨ã¯ï¼Ÿ</h3>
            <p><strong>k-Nearest Neighbors:</strong> A simple, instance-based learning algorithm that classifies new data points based on the majority class of their k nearest neighbors in the training data.</p>
            <p><strong>kè¿‘å‚æ³•:</strong> è¨“ç·´ãƒ‡ãƒ¼ã‚¿å†…ã®kå€‹ã®æœ€è¿‘å‚ã®å¤šæ•°ã‚¯ãƒ©ã‚¹ã«åŸºã¥ã„ã¦æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’åˆ†é¡ã™ã‚‹ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãƒ™ãƒ¼ã‚¹å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€‚</p>
        </div>

        <div class="bilingual">
            <div class="english">
                <h3>The "You Are Your Neighbors" Algorithm</h3>

                <h4>How k-NN Works</h4>
                <div class="code-box">k-NN ALGORITHM (k=3 example)

Step 1: Store training data (no actual "training")
â”œâ”€ Keep all labeled examples in memory
â””â”€ No model building phase

Step 2: Receive new data point to classify
â””â”€ Example: New house to price

Step 3: Calculate distances to ALL training points
â”œâ”€ Use Euclidean distance (or other metric)
â””â”€ d = âˆš[(xâ‚-xâ‚‚)Â² + (yâ‚-yâ‚‚)Â²]

Step 4: Find k nearest neighbors
â”œâ”€ Sort by distance
â””â”€ Select k closest points (e.g., k=3)

Step 5: Vote for classification
â”œâ”€ Count class labels of k neighbors
â””â”€ Majority class wins

Example: Classify fruit (Apple or Orange?)
New fruit: [Weight=150g, Diameter=8cm]

Nearest 3 neighbors (k=3):
1. Apple   - distance 2.1
2. Apple   - distance 2.3
3. Orange  - distance 2.8

Vote: 2 Apples, 1 Orange
â†’ Prediction: Apple</div>

                <h4>Choosing K</h4>
                <ul>
                    <li><strong>K = 1:</strong> Sensitive to noise, overfitting risk</li>
                    <li><strong>K = 3 or 5:</strong> Good starting point</li>
                    <li><strong>K = N (all data):</strong> Always predicts majority class, underfitting</li>
                    <li><strong>Odd K:</strong> Prevents ties in binary classification</li>
                    <li><strong>Rule of thumb:</strong> K = âˆšN where N is number of samples</li>
                </ul>

                <h4>Distance Metrics</h4>
                <ul>
                    <li><strong>Euclidean:</strong> Straight-line distance (most common)</li>
                    <li><strong>Manhattan:</strong> Sum of absolute differences</li>
                    <li><strong>Minkowski:</strong> Generalization of above</li>
                    <li><strong>Hamming:</strong> For categorical features</li>
                </ul>

                <h4>Advantages:</h4>
                <ul>
                    <li><strong>Simple:</strong> Easy to understand and implement</li>
                    <li><strong>No training:</strong> Just store data</li>
                    <li><strong>New data:</strong> Immediately uses new training examples</li>
                    <li><strong>Multi-class:</strong> Naturally handles multiple classes</li>
                    <li><strong>Non-linear:</strong> Can capture complex decision boundaries</li>
                </ul>

                <h4>Disadvantages:</h4>
                <ul>
                    <li><strong>Slow prediction:</strong> Must calculate distance to all points</li>
                    <li><strong>Memory intensive:</strong> Stores all training data</li>
                    <li><strong>Curse of dimensionality:</strong> Poor performance with many features</li>
                    <li><strong>Scale sensitive:</strong> Requires feature normalization</li>
                    <li><strong>Imbalanced data:</strong> Dominated by majority class</li>
                </ul>
            </div>
            <div class="japanese">
                <h3>ã€Œã‚ãªãŸã¯éš£äººã®ã‚ˆã†ã«ãªã‚‹ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </h3>

                <h4>k-NNã®ä»•çµ„ã¿</h4>
                <div class="code-box">k-NNã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆk=3ã®ä¾‹ï¼‰

ã‚¹ãƒ†ãƒƒãƒ—1: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆå®Ÿéš›ã®ã€Œè¨“ç·´ã€ãªã—ï¼‰
â”œâ”€ ã™ã¹ã¦ã®ãƒ©ãƒ™ãƒ«ä»˜ãä¾‹ã‚’ãƒ¡ãƒ¢ãƒªã«ä¿æŒ
â””â”€ ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ãƒ•ã‚§ãƒ¼ã‚ºãªã—

ã‚¹ãƒ†ãƒƒãƒ—2: åˆ†é¡ã™ã‚‹æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’å—ã‘å–ã‚‹
â””â”€ ä¾‹: ä¾¡æ ¼ã‚’ä»˜ã‘ã‚‹æ–°ã—ã„ä½å®…

ã‚¹ãƒ†ãƒƒãƒ—3: ã™ã¹ã¦ã®è¨“ç·´ãƒã‚¤ãƒ³ãƒˆã¾ã§ã®è·é›¢ã‚’è¨ˆç®—
â”œâ”€ ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ï¼ˆã¾ãŸã¯ä»–ã®ãƒ¡ãƒˆãƒªãƒƒã‚¯ï¼‰ã‚’ä½¿ç”¨
â””â”€ d = âˆš[(xâ‚-xâ‚‚)Â² + (yâ‚-yâ‚‚)Â²]

ã‚¹ãƒ†ãƒƒãƒ—4: kå€‹ã®æœ€è¿‘å‚ã‚’è¦‹ã¤ã‘ã‚‹
â”œâ”€ è·é›¢ã§ã‚½ãƒ¼ãƒˆ
â””â”€ kå€‹ã®æœ€ã‚‚è¿‘ã„ãƒã‚¤ãƒ³ãƒˆã‚’é¸æŠï¼ˆä¾‹: k=3ï¼‰

ã‚¹ãƒ†ãƒƒãƒ—5: åˆ†é¡ã®ãŸã‚ã«æŠ•ç¥¨
â”œâ”€ kå€‹ã®éš£äººã®ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ã‚’æ•°ãˆã‚‹
â””â”€ å¤šæ•°ã‚¯ãƒ©ã‚¹ãŒå‹ã¤

ä¾‹: æœç‰©ã‚’åˆ†é¡ï¼ˆãƒªãƒ³ã‚´ã‹ã‚ªãƒ¬ãƒ³ã‚¸ã‹ï¼Ÿï¼‰
æ–°ã—ã„æœç‰©: [é‡é‡=150gã€ç›´å¾„=8cm]

æœ€è¿‘å‚3å€‹ï¼ˆk=3ï¼‰:
1. ãƒªãƒ³ã‚´   - è·é›¢ 2.1
2. ãƒªãƒ³ã‚´   - è·é›¢ 2.3
3. ã‚ªãƒ¬ãƒ³ã‚¸  - è·é›¢ 2.8

æŠ•ç¥¨: ãƒªãƒ³ã‚´2å€‹ã€ã‚ªãƒ¬ãƒ³ã‚¸1å€‹
â†’ äºˆæ¸¬: ãƒªãƒ³ã‚´</div>

                <h4>Kã®é¸æŠ</h4>
                <ul>
                    <li><strong>K = 1:</strong> ãƒã‚¤ã‚ºã«æ•æ„Ÿã€éå­¦ç¿’ãƒªã‚¹ã‚¯</li>
                    <li><strong>K = 3ã¾ãŸã¯5:</strong> è‰¯ã„å‡ºç™ºç‚¹</li>
                    <li><strong>K = Nï¼ˆã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ï¼‰:</strong> å¸¸ã«å¤šæ•°ã‚¯ãƒ©ã‚¹ã‚’äºˆæ¸¬ã€æœªå­¦ç¿’</li>
                    <li><strong>å¥‡æ•°K:</strong> äºŒå€¤åˆ†é¡ã§ã®åŒç‚¹ã‚’é˜²ã</li>
                    <li><strong>çµŒé¨“å‰‡:</strong> K = âˆšNã€Nã¯ã‚µãƒ³ãƒ—ãƒ«æ•°</li>
                </ul>

                <h4>è·é›¢ãƒ¡ãƒˆãƒªãƒƒã‚¯</h4>
                <ul>
                    <li><strong>ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰:</strong> ç›´ç·šè·é›¢ï¼ˆæœ€ã‚‚ä¸€èˆ¬çš„ï¼‰</li>
                    <li><strong>ãƒãƒ³ãƒãƒƒã‚¿ãƒ³:</strong> çµ¶å¯¾å·®ã®åˆè¨ˆ</li>
                    <li><strong>ãƒŸãƒ³ã‚³ãƒ•ã‚¹ã‚­ãƒ¼:</strong> ä¸Šè¨˜ã®ä¸€èˆ¬åŒ–</li>
                    <li><strong>ãƒãƒŸãƒ³ã‚°:</strong> ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´é‡ç”¨</li>
                </ul>

                <h4>åˆ©ç‚¹:</h4>
                <ul>
                    <li><strong>ã‚·ãƒ³ãƒ—ãƒ«:</strong> ç†è§£ã¨å®Ÿè£…ãŒå®¹æ˜“</li>
                    <li><strong>è¨“ç·´ãªã—:</strong> ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã™ã‚‹ã ã‘</li>
                    <li><strong>æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿:</strong> æ–°ã—ã„è¨“ç·´ä¾‹ã‚’ã™ãã«ä½¿ç”¨</li>
                    <li><strong>å¤šã‚¯ãƒ©ã‚¹:</strong> è¤‡æ•°ã‚¯ãƒ©ã‚¹ã‚’è‡ªç„¶ã«å‡¦ç†</li>
                    <li><strong>éç·šå½¢:</strong> è¤‡é›‘ãªæ±ºå®šå¢ƒç•Œã‚’æ‰ãˆã‚‰ã‚Œã‚‹</li>
                </ul>

                <h4>æ¬ ç‚¹:</h4>
                <ul>
                    <li><strong>äºˆæ¸¬ãŒé…ã„:</strong> ã™ã¹ã¦ã®ãƒã‚¤ãƒ³ãƒˆã¾ã§ã®è·é›¢ã‚’è¨ˆç®—ã™ã‚‹å¿…è¦</li>
                    <li><strong>ãƒ¡ãƒ¢ãƒªé›†ç´„çš„:</strong> ã™ã¹ã¦ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜</li>
                    <li><strong>æ¬¡å…ƒã®å‘ªã„:</strong> å¤šãã®ç‰¹å¾´é‡ã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒæ‚ªã„</li>
                    <li><strong>ã‚¹ã‚±ãƒ¼ãƒ«ã«æ•æ„Ÿ:</strong> ç‰¹å¾´é‡ã®æ­£è¦åŒ–ãŒå¿…è¦</li>
                    <li><strong>ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿:</strong> å¤šæ•°ã‚¯ãƒ©ã‚¹ã«æ”¯é…ã•ã‚Œã‚‹</li>
                </ul>
            </div>
        </div>

        <h2>4. Support Vector Machines (SVM) / ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚·ãƒ³</h2>

        <div class="definition-box">
            <h3>What is SVM? / SVMã¨ã¯ï¼Ÿ</h3>
            <p><strong>Support Vector Machine:</strong> A powerful classification algorithm that finds the optimal hyperplane (decision boundary) that maximizes the margin between different classes.</p>
            <p><strong>ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚·ãƒ³:</strong> ç•°ãªã‚‹ã‚¯ãƒ©ã‚¹é–“ã®ãƒãƒ¼ã‚¸ãƒ³ã‚’æœ€å¤§åŒ–ã™ã‚‹æœ€é©ãªè¶…å¹³é¢ï¼ˆæ±ºå®šå¢ƒç•Œï¼‰ã‚’è¦‹ã¤ã‘ã‚‹å¼·åŠ›ãªåˆ†é¡ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€‚</p>
        </div>

        <div class="bilingual">
            <div class="english">
                <h3>Maximum Margin Classification</h3>

                <h4>Core Concept: The Margin</h4>
                <div class="code-box">VISUALIZING SVM (2D example)

Class A (â—)              Decision Boundary (|)              Class B (â– )

    â—                              |                             â– 
      â—                            |                           â– 
        â—                          |                         â– 
    â—     â—                        |                       â– 
      â—                            |                     â–    â– 
                                   |
        â† Support Vectors          |          Support Vectors â†’
           (closest points)        |          (closest points)
                                   |
           |â† Margin â†’|â† Margin â†’|
           (maximum distance from boundary)

Goal: Find the line that maximizes the margin
Margin = Distance from boundary to nearest points of each class</div>

                <h4>How SVM Works</h4>
                <ol>
                    <li><strong>Find Decision Boundary:</strong> Line (2D), plane (3D), or hyperplane (>3D) that separates classes</li>
                    <li><strong>Maximize Margin:</strong> Choose boundary that maximizes distance to nearest points</li>
                    <li><strong>Support Vectors:</strong> The critical points closest to boundary that "support" it</li>
                    <li><strong>Make Predictions:</strong> New points classified based on which side of boundary they fall</li>
                </ol>

                <h4>The Kernel Trick</h4>
                <p>What if data isn't linearly separable? Use kernels to transform data into higher dimensions!</p>

                <ul>
                    <li><strong>Linear Kernel:</strong> Standard straight line/plane</li>
                    <li><strong>Polynomial Kernel:</strong> Curved boundaries</li>
                    <li><strong>RBF (Radial Basis Function):</strong> Complex circular/curved boundaries</li>
                    <li><strong>Sigmoid Kernel:</strong> Similar to neural networks</li>
                </ul>

                <div class="code-box">Example: XOR Problem (not linearly separable)

2D Space (can't draw straight line):
  â–     â—
  â—    â– 

Transform to 3D (now can separate with plane):
     â—
  â–      â– 
     â—</div>

                <h4>Advantages:</h4>
                <ul>
                    <li><strong>Effective in high dimensions:</strong> Works well with many features</li>
                    <li><strong>Memory efficient:</strong> Only uses support vectors</li>
                    <li><strong>Versatile:</strong> Different kernels for different patterns</li>
                    <li><strong>Robust:</strong> Works well with clear margin of separation</li>
                </ul>

                <h4>Disadvantages:</h4>
                <ul>
                    <li><strong>Slow with large datasets:</strong> Training time increases with data size</li>
                    <li><strong>Kernel choice:</strong> Selecting right kernel can be tricky</li>
                    <li><strong>No probability estimates:</strong> Only gives class predictions</li>
                    <li><strong>Sensitive to noise:</strong> Outliers can affect boundary</li>
                </ul>
            </div>
            <div class="japanese">
                <h3>æœ€å¤§ãƒãƒ¼ã‚¸ãƒ³åˆ†é¡</h3>

                <h4>ã‚³ã‚¢æ¦‚å¿µ: ãƒãƒ¼ã‚¸ãƒ³</h4>
                <div class="code-box">SVMã®å¯è¦–åŒ–ï¼ˆ2Dä¾‹ï¼‰

ã‚¯ãƒ©ã‚¹Aï¼ˆâ—ï¼‰              æ±ºå®šå¢ƒç•Œï¼ˆ|ï¼‰              ã‚¯ãƒ©ã‚¹Bï¼ˆâ– ï¼‰

    â—                              |                             â– 
      â—                            |                           â– 
        â—                          |                         â– 
    â—     â—                        |                       â– 
      â—                            |                     â–    â– 
                                   |
        â† ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼         |          ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ â†’
           (æœ€ã‚‚è¿‘ã„ãƒã‚¤ãƒ³ãƒˆ)      |          (æœ€ã‚‚è¿‘ã„ãƒã‚¤ãƒ³ãƒˆ)
                                   |
           |â† ãƒãƒ¼ã‚¸ãƒ³ â†’|â† ãƒãƒ¼ã‚¸ãƒ³ â†’|
           (å¢ƒç•Œã‹ã‚‰ã®æœ€å¤§è·é›¢)

ç›®æ¨™: ãƒãƒ¼ã‚¸ãƒ³ã‚’æœ€å¤§åŒ–ã™ã‚‹ç·šã‚’è¦‹ã¤ã‘ã‚‹
ãƒãƒ¼ã‚¸ãƒ³ = å¢ƒç•Œã‹ã‚‰å„ã‚¯ãƒ©ã‚¹ã®æœ€ã‚‚è¿‘ã„ãƒã‚¤ãƒ³ãƒˆã¾ã§ã®è·é›¢</div>

                <h4>SVMã®ä»•çµ„ã¿</h4>
                <ol>
                    <li><strong>æ±ºå®šå¢ƒç•Œã‚’è¦‹ã¤ã‘ã‚‹:</strong> ã‚¯ãƒ©ã‚¹ã‚’åˆ†é›¢ã™ã‚‹ç·šï¼ˆ2Dï¼‰ã€å¹³é¢ï¼ˆ3Dï¼‰ã€ã¾ãŸã¯è¶…å¹³é¢ï¼ˆ>3Dï¼‰</li>
                    <li><strong>ãƒãƒ¼ã‚¸ãƒ³ã‚’æœ€å¤§åŒ–:</strong> æœ€ã‚‚è¿‘ã„ãƒã‚¤ãƒ³ãƒˆã¾ã§ã®è·é›¢ã‚’æœ€å¤§åŒ–ã™ã‚‹å¢ƒç•Œã‚’é¸æŠ</li>
                    <li><strong>ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼:</strong> å¢ƒç•Œã‚’ã€Œã‚µãƒãƒ¼ãƒˆã€ã™ã‚‹å¢ƒç•Œã«æœ€ã‚‚è¿‘ã„é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ</li>
                    <li><strong>äºˆæ¸¬ã‚’è¡Œã†:</strong> æ–°ã—ã„ãƒã‚¤ãƒ³ãƒˆã¯å¢ƒç•Œã®ã©ã¡ã‚‰å´ã«è½ã¡ã‚‹ã‹ã«åŸºã¥ã„ã¦åˆ†é¡ã•ã‚Œã‚‹</li>
                </ol>

                <h4>ã‚«ãƒ¼ãƒãƒ«ãƒˆãƒªãƒƒã‚¯</h4>
                <p>ãƒ‡ãƒ¼ã‚¿ãŒç·šå½¢åˆ†é›¢å¯èƒ½ã§ãªã„å ´åˆã¯ï¼Ÿã‚«ãƒ¼ãƒãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ã‚ˆã‚Šé«˜ã„æ¬¡å…ƒã«å¤‰æ›ï¼</p>

                <ul>
                    <li><strong>ç·šå½¢ã‚«ãƒ¼ãƒãƒ«:</strong> æ¨™æº–çš„ãªç›´ç·š/å¹³é¢</li>
                    <li><strong>å¤šé …å¼ã‚«ãƒ¼ãƒãƒ«:</strong> æ›²ç·šå¢ƒç•Œ</li>
                    <li><strong>RBFï¼ˆæ”¾å°„åŸºåº•é–¢æ•°ï¼‰:</strong> è¤‡é›‘ãªå††å½¢/æ›²ç·šå¢ƒç•Œ</li>
                    <li><strong>ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰ã‚«ãƒ¼ãƒãƒ«:</strong> ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«é¡ä¼¼</li>
                </ul>

                <div class="code-box">ä¾‹: XORå•é¡Œï¼ˆç·šå½¢åˆ†é›¢ä¸å¯èƒ½ï¼‰

2Dç©ºé–“ï¼ˆç›´ç·šã‚’å¼•ã‘ãªã„ï¼‰:
  â–     â—
  â—    â– 

3Dã«å¤‰æ›ï¼ˆå¹³é¢ã§åˆ†é›¢å¯èƒ½ï¼‰:
     â—
  â–      â– 
     â—</div>

                <h4>åˆ©ç‚¹:</h4>
                <ul>
                    <li><strong>é«˜æ¬¡å…ƒã§åŠ¹æœçš„:</strong> å¤šãã®ç‰¹å¾´é‡ã§ã†ã¾ãæ©Ÿèƒ½</li>
                    <li><strong>ãƒ¡ãƒ¢ãƒªåŠ¹ç‡:</strong> ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ã®ã¿ã‚’ä½¿ç”¨</li>
                    <li><strong>å¤šç”¨é€”:</strong> ç•°ãªã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãŸã‚ã®ç•°ãªã‚‹ã‚«ãƒ¼ãƒãƒ«</li>
                    <li><strong>å …ç‰¢:</strong> æ˜ç¢ºãªåˆ†é›¢ãƒãƒ¼ã‚¸ãƒ³ã§ã†ã¾ãæ©Ÿèƒ½</li>
                </ul>

                <h4>æ¬ ç‚¹:</h4>
                <ul>
                    <li><strong>å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§é…ã„:</strong> è¨“ç·´æ™‚é–“ãŒãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã¨ã¨ã‚‚ã«å¢—åŠ </li>
                    <li><strong>ã‚«ãƒ¼ãƒãƒ«é¸æŠ:</strong> é©åˆ‡ãªã‚«ãƒ¼ãƒãƒ«ã®é¸æŠãŒé›£ã—ã„å ´åˆãŒã‚ã‚‹</li>
                    <li><strong>ç¢ºç‡æ¨å®šãªã—:</strong> ã‚¯ãƒ©ã‚¹äºˆæ¸¬ã®ã¿ã‚’æä¾›</li>
                    <li><strong>ãƒã‚¤ã‚ºã«æ•æ„Ÿ:</strong> å¤–ã‚Œå€¤ãŒå¢ƒç•Œã«å½±éŸ¿ã‚’ä¸ãˆã‚‹å¯èƒ½æ€§</li>
                </ul>
            </div>
        </div>

        <h2>5. Comparing Classification Algorithms / åˆ†é¡ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ¯”è¼ƒ</h2>

        <div class="highlight-box">
            <h3>Algorithm Comparison Table</h3>
            <div class="code-box">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Algorithm       â”‚ Speed        â”‚Interpretable â”‚ Accuracy     â”‚ Best For     â”‚
â”‚ ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ     â”‚ é€Ÿåº¦         â”‚è§£é‡ˆå¯èƒ½æ€§    â”‚ ç²¾åº¦         â”‚ æœ€é©ç”¨é€”     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Logistic        â”‚ Fast         â”‚ High         â”‚ Good         â”‚ Linear       â”‚
â”‚ Regression      â”‚ é€Ÿã„         â”‚ é«˜ã„         â”‚ è‰¯ã„         â”‚ ç·šå½¢å•é¡Œ     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Decision Tree   â”‚ Fast         â”‚ Very High    â”‚ Good         â”‚ Mixed data   â”‚
â”‚ æ±ºå®šæœ¨          â”‚ é€Ÿã„         â”‚ éå¸¸ã«é«˜ã„   â”‚ è‰¯ã„         â”‚ æ··åˆãƒ‡ãƒ¼ã‚¿   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Random Forest   â”‚ Medium       â”‚ Low          â”‚ Excellent    â”‚ General      â”‚
â”‚ ãƒ©ãƒ³ãƒ€ãƒ         â”‚ ä¸­ç¨‹åº¦       â”‚ ä½ã„         â”‚ å„ªç§€         â”‚ æ±ç”¨         â”‚
â”‚ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ      â”‚              â”‚              â”‚              â”‚              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ k-NN            â”‚ Slow         â”‚ Medium       â”‚ Good         â”‚ Small data   â”‚
â”‚ kè¿‘å‚æ³•         â”‚ é…ã„         â”‚ ä¸­ç¨‹åº¦       â”‚ è‰¯ã„         â”‚ å°ãƒ‡ãƒ¼ã‚¿     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ SVM             â”‚ Slow (large) â”‚ Low          â”‚ Excellent    â”‚ High-dim     â”‚
â”‚ SVM             â”‚é…ã„(å¤§è¦æ¨¡)  â”‚ ä½ã„         â”‚ å„ªç§€         â”‚ é«˜æ¬¡å…ƒ       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</div>
        </div>

        <h2>6. Real-World Application / å®Ÿä¸–ç•Œã®å¿œç”¨</h2>

        <div class="highlight-box">
            <h3>Amazon: Product Recommendation System</h3>
            <h4>Link: <a href="https://www.amazon.com" target="_blank">https://www.amazon.com</a></h4>

            <div class="bilingual">
                <div class="english">
                    <p><strong>Challenge:</strong> Recommend relevant products to millions of customers from billions of items.</p>

                    <p><strong>Classification Approach:</strong></p>
                    <ul>
                        <li><strong>Problem Type:</strong> Binary classification - Will user click/buy this product? (Yes/No)</li>
                        <li><strong>Algorithm:</strong> Ensemble of Random Forests, Gradient Boosted Trees</li>
                        <li><strong>Features Used:</strong>
                            <ul>
                                <li>User browsing history</li>
                                <li>Past purchases</li>
                                <li>Product categories viewed</li>
                                <li>Time spent on product pages</li>
                                <li>Reviews read and written</li>
                                <li>Search queries</li>
                                <li>Similar users' behavior (collaborative filtering)</li>
                                <li>Product attributes (price, category, brand)</li>
                                <li>Current session behavior</li>
                            </ul>
                        </li>
                    </ul>

                    <p><strong>Multiple Classification Models:</strong></p>
                    <ul>
                        <li><strong>"Customers who bought this also bought":</strong> Item-to-item collaborative filtering with classification</li>
                        <li><strong>"Recommended for you":</strong> Personalized recommendations using user features</li>
                        <li><strong>"Frequently bought together":</strong> Association rules + classification</li>
                        <li><strong>Search results ranking:</strong> Classifying relevance of each product to query</li>
                    </ul>

                    <p><strong>Why Random Forests?</strong></p>
                    <ul>
                        <li>Handles millions of features well</li>
                        <li>Provides feature importance (what matters most?)</li>
                        <li>Resistant to overfitting with large feature sets</li>
                        <li>Can combine multiple data types</li>
                        <li>Good accuracy with minimal tuning</li>
                    </ul>

                    <p><strong>Impact:</strong> 35% of Amazon's revenue comes from recommendation engine. Billions in sales attributed to ML-powered recommendations.</p>
                </div>
                <div class="japanese">
                    <p><strong>èª²é¡Œ:</strong> æ•°åå„„ã®ã‚¢ã‚¤ãƒ†ãƒ ã‹ã‚‰æ•°ç™¾ä¸‡ã®é¡§å®¢ã«é–¢é€£è£½å“ã‚’æ¨è–¦ã™ã‚‹ã€‚</p>

                    <p><strong>åˆ†é¡ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:</strong></p>
                    <ul>
                        <li><strong>å•é¡Œã‚¿ã‚¤ãƒ—:</strong> äºŒå€¤åˆ†é¡ - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã“ã®è£½å“ã‚’ã‚¯ãƒªãƒƒã‚¯/è³¼å…¥ã™ã‚‹ã‹ï¼Ÿï¼ˆã¯ã„/ã„ã„ãˆï¼‰</li>
                        <li><strong>ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ :</strong> ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã€å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°æœ¨ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«</li>
                        <li><strong>ä½¿ç”¨ã•ã‚Œã‚‹ç‰¹å¾´é‡:</strong>
                            <ul>
                                <li>ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é–²è¦§å±¥æ­´</li>
                                <li>éå»ã®è³¼å…¥</li>
                                <li>é–²è¦§ã—ãŸè£½å“ã‚«ãƒ†ã‚´ãƒª</li>
                                <li>è£½å“ãƒšãƒ¼ã‚¸ã«è²»ã‚„ã—ãŸæ™‚é–“</li>
                                <li>èª­ã‚“ã ãƒ»æ›¸ã„ãŸãƒ¬ãƒ“ãƒ¥ãƒ¼</li>
                                <li>æ¤œç´¢ã‚¯ã‚¨ãƒª</li>
                                <li>é¡ä¼¼ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¡Œå‹•ï¼ˆå”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰</li>
                                <li>è£½å“å±æ€§ï¼ˆä¾¡æ ¼ã€ã‚«ãƒ†ã‚´ãƒªã€ãƒ–ãƒ©ãƒ³ãƒ‰ï¼‰</li>
                                <li>ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³è¡Œå‹•</li>
                            </ul>
                        </li>
                    </ul>

                    <p><strong>è¤‡æ•°ã®åˆ†é¡ãƒ¢ãƒ‡ãƒ«:</strong></p>
                    <ul>
                        <li><strong>ã€Œã“ã‚Œã‚’è²·ã£ãŸäººã¯ã“ã‚“ãªå•†å“ã‚‚è²·ã£ã¦ã„ã¾ã™ã€:</strong> åˆ†é¡ã‚’ä¼´ã†ã‚¢ã‚¤ãƒ†ãƒ é–“å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°</li>
                        <li><strong>ã€Œã‚ãªãŸã¸ã®ãŠã™ã™ã‚ã€:</strong> ãƒ¦ãƒ¼ã‚¶ãƒ¼ç‰¹å¾´é‡ã‚’ä½¿ç”¨ã—ãŸãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸæ¨è–¦</li>
                        <li><strong>ã€Œã‚ˆãä¸€ç·’ã«è³¼å…¥ã•ã‚Œã¦ã„ã‚‹å•†å“ã€:</strong> ã‚¢ã‚½ã‚·ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ« + åˆ†é¡</li>
                        <li><strong>æ¤œç´¢çµæœã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°:</strong> ã‚¯ã‚¨ãƒªã«å¯¾ã™ã‚‹å„è£½å“ã®é–¢é€£æ€§ã‚’åˆ†é¡</li>
                    </ul>

                    <p><strong>ãªãœãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆï¼Ÿ</strong></p>
                    <ul>
                        <li>æ•°ç™¾ä¸‡ã®ç‰¹å¾´é‡ã‚’ã†ã¾ãå‡¦ç†</li>
                        <li>ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’æä¾›ï¼ˆä½•ãŒæœ€ã‚‚é‡è¦ã‹ï¼Ÿï¼‰</li>
                        <li>å¤§è¦æ¨¡ãªç‰¹å¾´é‡ã‚»ãƒƒãƒˆã§ã‚‚éå­¦ç¿’ã«è€æ€§ãŒã‚ã‚‹</li>
                        <li>è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã‚’çµ„ã¿åˆã‚ã›ã‚‰ã‚Œã‚‹</li>
                        <li>æœ€å°é™ã®èª¿æ•´ã§è‰¯å¥½ãªç²¾åº¦</li>
                    </ul>

                    <p><strong>å½±éŸ¿:</strong> Amazonã®åç›Šã®35%ãŒæ¨è–¦ã‚¨ãƒ³ã‚¸ãƒ³ã‹ã‚‰æ¥ã¦ã„ã¾ã™ã€‚MLé§†å‹•ã®æ¨è–¦ã«ã‚ˆã‚Šæ•°åå„„ãƒ‰ãƒ«ã®å£²ä¸ŠãŒè²¢çŒ®ã•ã‚Œã¦ã„ã¾ã™ã€‚</p>
                </div>
            </div>
        </div>

        <div class="study-tip">
            <h3>Study Tips for Classification Algorithms</h3>
            <ul>
                <li>Create a comparison table of all algorithms in your own words</li>
                <li>Draw a decision tree by hand for a simple dataset</li>
                <li>Understand why Random Forests reduce overfitting</li>
                <li>Practice calculating k-NN predictions with small examples</li>
                <li>Visualize SVM margin concept with sketches</li>
                <li>Know when to use each algorithm (strengths/weaknesses)</li>
                <li>Understand ensemble methods and why they work</li>
            </ul>
        </div>

        <h2>ğŸ“ Test Questions / ãƒ†ã‚¹ãƒˆå•é¡Œ</h2>

        <div class="test-question">
            <h4>TEST QUESTION 1: Multiple Choice</h4>
            <p><strong>What is the main advantage of Random Forest over a single Decision Tree?</strong></p>
            <ol type="A">
                <li>Faster training time</li>
                <li>More interpretable results</li>
                <li>Reduces overfitting by averaging multiple trees</li>
                <li>Requires less memory</li>
            </ol>
            <p><strong>Answer:</strong> C - Random Forest reduces overfitting because individual trees may overfit differently, but averaging cancels out errors.</p>
        </div>

        <div class="test-question">
            <h4>TEST QUESTION 2: Multiple Choice</h4>
            <p><strong>In k-NN with k=5, a new data point has 3 neighbors of Class A and 2 neighbors of Class B. What will be the predicted class?</strong></p>
            <ol type="A">
                <li>Class A (majority vote)</li>
                <li>Class B</li>
                <li>Cannot determine</li>
                <li>Average of both classes</li>
            </ol>
            <p><strong>Answer:</strong> A - k-NN uses majority voting, so Class A wins with 3 votes vs 2.</p>
        </div>

        <div class="test-question">
            <h4>TEST QUESTION 3: Multiple Choice</h4>
            <p><strong>What is a "support vector" in SVM?</strong></p>
            <ol type="A">
                <li>Any data point in the training set</li>
                <li>The decision boundary line</li>
                <li>The data points closest to the decision boundary</li>
                <li>The center point of each class</li>
            </ol>
            <p><strong>Answer:</strong> C - Support vectors are the critical points closest to the decision boundary that define the margin.</p>
        </div>

        <div class="test-question">
            <h4>TEST QUESTION 4: Multiple Choice</h4>
            <p><strong>Which algorithm would be BEST for a problem with clear linear separation between classes?</strong></p>
            <ol type="A">
                <li>Decision Tree</li>
                <li>Random Forest</li>
                <li>Logistic Regression</li>
                <li>k-NN with k=1</li>
            </ol>
            <p><strong>Answer:</strong> C - Logistic Regression is simple, fast, and effective for linearly separable data.</p>
        </div>

        <div class="test-question">
            <h4>TEST QUESTION 5: Multiple Choice</h4>
            <p><strong>What is the purpose of the "kernel trick" in SVM?</strong></p>
            <ol type="A">
                <li>To speed up training</li>
                <li>To reduce memory usage</li>
                <li>To handle non-linearly separable data by transforming to higher dimensions</li>
                <li>To improve interpretability</li>
            </ol>
            <p><strong>Answer:</strong> C - The kernel trick transforms data into higher dimensions where it becomes linearly separable.</p>
        </div>

        <div class="test-question">
            <h4>TEST QUESTION 6: Short Answer</h4>
            <p><strong>Explain how a Decision Tree makes a prediction. Use a simple example like predicting whether to play tennis based on weather.</strong></p>
            <p><strong>Sample Answer:</strong></p>
            <p>A Decision Tree makes predictions by following a series of yes/no questions from the root to a leaf node.</p>
            <p><strong>Example: Play Tennis?</strong></p>
            <ol>
                <li><strong>Root Node:</strong> "Is it sunny?"
                    <ul>
                        <li>If YES â†’ Go to next question: "Is humidity high?"
                            <ul>
                                <li>If YES â†’ DON'T PLAY (too humid)</li>
                                <li>If NO â†’ PLAY</li>
                            </ul>
                        </li>
                        <li>If NO â†’ Go to next question: "Is it windy?"
                            <ul>
                                <li>If YES â†’ DON'T PLAY (too windy)</li>
                                <li>If NO â†’ PLAY</li>
                            </ul>
                        </li>
                    </ul>
                </li>
            </ol>
            <p>The tree follows the path from root to leaf based on the feature values, and the leaf node gives the final prediction.</p>
        </div>

        <div class="test-question">
            <h4>TEST QUESTION 7: Short Answer</h4>
            <p><strong>Why does Random Forest use bootstrap sampling and random feature selection? What problem does this solve?</strong></p>
            <p><strong>Sample Answer:</strong></p>
            <p><strong>Bootstrap Sampling:</strong> Each tree is trained on a different random sample of the data (with replacement). This creates diversity among trees because each one sees slightly different training examples.</p>
            <p><strong>Random Feature Selection:</strong> At each split, only a random subset of features is considered. This prevents all trees from making the same decisions.</p>
            <p><strong>Problem Solved:</strong> These techniques solve the overfitting problem. If all trees were identical and trained on the same data, they would make the same mistakes. By creating diverse trees through randomness, their errors are different and cancel out when averaged, resulting in better generalization to new data.</p>
        </div>

        <div class="test-question">
            <h4>TEST QUESTION 8: Short Answer</h4>
            <p><strong>Describe the complete process of how k-NN (k=3) would classify a new data point. Include all steps.</strong></p>
            <p><strong>Sample Answer:</strong></p>
            <ol>
                <li><strong>Store Training Data:</strong> k-NN doesn't build a model; it stores all training examples in memory.</li>
                <li><strong>Receive New Point:</strong> A new unlabeled data point arrives for classification.</li>
                <li><strong>Calculate Distances:</strong> Compute the distance from the new point to every training point using a distance metric (e.g., Euclidean distance).</li>
                <li><strong>Find k Nearest:</strong> Sort all distances and select the 3 closest training points (k=3).</li>
                <li><strong>Majority Vote:</strong> Look at the class labels of these 3 neighbors. Count the votes for each class.</li>
                <li><strong>Predict:</strong> Assign the new point to the class with the most votes among the 3 neighbors.</li>
            </ol>
            <p><strong>Example:</strong> New point has 2 neighbors labeled "Cat" and 1 labeled "Dog" â†’ Prediction: "Cat"</p>
        </div>

        <div class="test-question">
            <h4>TEST QUESTION 9: Application Question</h4>
            <p><strong>Amazon uses Random Forests for product recommendations. Explain:</strong></p>
            <p><strong>a) What classification problem is Amazon solving?</strong></p>
            <p><strong>b) List 5 features they might use</strong></p>
            <p><strong>c) Why is Random Forest a good choice for this problem?</strong></p>
            <p><strong>Sample Answer:</strong></p>
            <p><strong>a) Classification Problem:</strong> Binary classification - predicting whether a user will click on or purchase a recommended product (Yes/No). For each user-product pair, predict: Will the user interact with this product?</p>
            <p><strong>b) Five Features:</strong></p>
            <ol>
                <li>User's past purchase history (categories bought)</li>
                <li>Browsing time spent on similar products</li>
                <li>User's search queries</li>
                <li>Products purchased by similar users (collaborative filtering)</li>
                <li>Product price relative to user's typical spending</li>
            </ol>
            <p><strong>c) Why Random Forest:</strong></p>
            <ul>
                <li>Handles millions of features (users have complex behavior patterns)</li>
                <li>Provides feature importance to understand what drives purchases</li>
                <li>Resistant to overfitting despite huge feature sets</li>
                <li>Works well with mixed data types (numerical prices, categorical product types)</li>
                <li>High accuracy with minimal parameter tuning, crucial for production systems</li>
            </ul>
        </div>

        <div class="test-question">
            <h4>TEST QUESTION 10: Essay Question</h4>
            <p><strong>Compare Decision Trees, Random Forests, k-NN, and SVM. For each algorithm, explain:</strong></p>
            <ul>
                <li>How it makes predictions</li>
                <li>One major advantage</li>
                <li>One major disadvantage</li>
                <li>A scenario where it would be the best choice</li>
            </ul>
            <p><strong>Sample Answer:</strong></p>

            <p><strong>Decision Tree:</strong></p>
            <p><strong>Prediction:</strong> Follows a tree of yes/no questions from root to leaf node, where each internal node tests a feature and each leaf provides a class label.</p>
            <p><strong>Advantage:</strong> Highly interpretable - you can visualize and explain exactly why a prediction was made.</p>
            <p><strong>Disadvantage:</strong> Prone to overfitting, creating overly complex trees that memorize training data.</p>
            <p><strong>Best For:</strong> When you need to explain decisions to non-technical stakeholders (e.g., medical diagnosis where doctors need to understand the reasoning).</p>

            <p><strong>Random Forest:</strong></p>
            <p><strong>Prediction:</strong> Builds many decision trees on random data samples, then combines their predictions through majority voting.</p>
            <p><strong>Advantage:</strong> Excellent accuracy and reduces overfitting by averaging diverse trees.</p>
            <p><strong>Disadvantage:</strong> Low interpretability - hard to understand 100+ trees working together.</p>
            <p><strong>Best For:</strong> General-purpose classification when accuracy is more important than interpretability (e.g., product recommendations, fraud detection).</p>

            <p><strong>k-NN:</strong></p>
            <p><strong>Prediction:</strong> Finds k nearest training examples and assigns the majority class among those neighbors.</p>
            <p><strong>Advantage:</strong> Simple to understand and implement; no training phase required.</p>
            <p><strong>Disadvantage:</strong> Slow prediction because it must calculate distance to all training points.</p>
            <p><strong>Best For:</strong> Small datasets where simplicity is valued or when you need to immediately incorporate new training examples without retraining (e.g., small-scale image recognition with limited data).</p>

            <p><strong>SVM:</strong></p>
            <p><strong>Prediction:</strong> Finds the optimal hyperplane that maximizes the margin between classes; new points are classified based on which side of the boundary they fall.</p>
            <p><strong>Advantage:</strong> Effective in high-dimensional spaces and with complex non-linear boundaries (using kernel trick).</p>
            <p><strong>Disadvantage:</strong> Slow to train on large datasets and choosing the right kernel can be challenging.</p>
            <p><strong>Best For:</strong> High-dimensional data with clear margins like text classification where you have thousands of features (words) but need to separate documents into categories.</p>

            <p><strong>Summary:</strong> Choose Decision Tree for interpretability, Random Forest for accuracy, k-NN for simplicity with small data, and SVM for high-dimensional problems with good separation.</p>
        </div>

        <h2>ğŸ¯ Key Takeaways / é‡è¦ãƒã‚¤ãƒ³ãƒˆ</h2>

        <div class="highlight-box">
            <h3>Remember These Core Concepts:</h3>
            <ul>
                <li><strong>Decision Trees:</strong> Tree of yes/no questions, interpretable but prone to overfitting</li>
                <li><strong>Random Forest:</strong> Ensemble of trees, reduces overfitting, excellent accuracy</li>
                <li><strong>k-NN:</strong> Majority vote of k nearest neighbors, simple but slow prediction</li>
                <li><strong>SVM:</strong> Maximizes margin between classes, good for high-dimensional data</li>
                <li><strong>Ensemble Methods:</strong> Combining multiple models improves performance</li>
                <li><strong>Trade-offs:</strong> Speed vs Accuracy vs Interpretability</li>
                <li><strong>Real-World:</strong> Amazon uses Random Forests for recommendations (35% of revenue!)</li>
            </ul>
        </div>

        <div class="navigation">
            <a href="../../index.html">â† Course Home</a>
            <a href="../week-10/lecture.html">â† Previous Week</a>
            <a href="slides.html">View Slides</a>
            <a href="assignment.html">Assignment</a>
            <a href="../week-12/lecture.html">Next Week â†’</a>
        </div>

        <footer style="margin-top: 50px; padding-top: 20px; border-top: 2px solid #e5e7eb; text-align: center; color: #666;">
            <p>Week 11 Lecture Notes - Introduction to AI and Data Science</p>
            <p>Chukyo University - 2025</p>
        </footer>
    </div>
</body>
</html>