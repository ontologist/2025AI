<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Week 13 Lecture Notes: Reinforcement Learning</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
            line-height: 1.6;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 40px;
            border-radius: 15px;
            box-shadow: 0 10px 40px rgba(0,0,0,0.2);
        }

        h1 {
            color: #2563eb;
            font-size: 2.5em;
            margin-bottom: 10px;
            text-align: center;
            border-bottom: 4px solid #7c3aed;
            padding-bottom: 20px;
        }

        h2 {
            color: #7c3aed;
            font-size: 2em;
            margin-top: 40px;
            margin-bottom: 20px;
            border-left: 6px solid #2563eb;
            padding-left: 15px;
        }

        h3 {
            color: #2563eb;
            font-size: 1.5em;
            margin-top: 30px;
            margin-bottom: 15px;
        }

        h4 {
            color: #555;
            font-size: 1.2em;
            margin-top: 20px;
            margin-bottom: 10px;
        }

        .subtitle {
            text-align: center;
            color: #666;
            font-size: 1.2em;
            margin-bottom: 30px;
        }

        .bilingual {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
            margin: 20px 0;
        }

        .english {
            background: #dbeafe;
            padding: 20px;
            border-radius: 10px;
            border-left: 4px solid #2563eb;
        }

        .japanese {
            background: #f3e8ff;
            padding: 20px;
            border-radius: 10px;
            border-left: 4px solid #7c3aed;
        }

        .highlight-box {
            background: #fef3cd;
            border-left: 5px solid #f59e0b;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
        }

        .definition-box {
            background: #e0f2fe;
            border: 2px solid #0ea5e9;
            padding: 20px;
            margin: 20px 0;
            border-radius: 10px;
        }

        .code-box {
            background: #f4f4f4;
            border: 1px solid #ddd;
            border-radius: 5px;
            padding: 20px;
            font-family: 'Courier New', monospace;
            margin: 20px 0;
            white-space: pre-wrap;
            overflow-x: auto;
        }

        .test-question {
            background: #fce7f3;
            border: 3px solid #ec4899;
            padding: 25px;
            margin: 25px 0;
            border-radius: 10px;
        }

        .test-question h4::before {
            content: "âš ï¸ ";
        }

        ul, ol {
            margin-left: 40px;
            margin-bottom: 20px;
        }

        li {
            margin-bottom: 10px;
        }

        p {
            margin-bottom: 15px;
        }

        .navigation {
            background: #e0e7ff;
            padding: 20px;
            border-radius: 10px;
            margin: 30px 0;
            text-align: center;
        }

        .navigation a {
            display: inline-block;
            margin: 10px;
            padding: 12px 24px;
            background: #2563eb;
            color: white;
            text-decoration: none;
            border-radius: 5px;
            transition: all 0.3s;
        }

        .navigation a:hover {
            background: #7c3aed;
            transform: translateY(-2px);
        }

        .key-terms {
            background: #f0fdf4;
            border-left: 5px solid #10b981;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
        }

        .study-tip {
            background: #fff7ed;
            border-left: 5px solid #f97316;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
        }

        @media (max-width: 768px) {
            .bilingual {
                grid-template-columns: 1fr;
            }

            .container {
                padding: 20px;
            }

            h1 {
                font-size: 2em;
            }

            h2 {
                font-size: 1.6em;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Week 13 Lecture Notes</h1>
        <p class="subtitle">Reinforcement Learning<br>å¼·åŒ–å­¦ç¿’</p>

        <div class="navigation">
            <a href="../../index.html">â† Course Home</a>
            <a href="../week-12/lecture.html">â† Previous Week</a>
            <a href="slides.html">View Slides</a>
            <a href="assignment.html">Assignment</a>
        </div>

        <h2>ğŸ“š Overview / æ¦‚è¦</h2>
        <div class="bilingual">
            <div class="english">
                <p>This week explores Reinforcement Learning (RL), a unique machine learning paradigm where agents learn to make decisions by interacting with an environment and receiving rewards. Unlike supervised learning (which uses labeled data) or unsupervised learning (which finds patterns), RL learns through trial, error, and feedback.</p>
                <p><strong>Learning Objectives:</strong></p>
                <ul>
                    <li>Understand the RL framework: agents, environments, actions, and rewards</li>
                    <li>Learn the exploration vs. exploitation tradeoff</li>
                    <li>Master Q-learning algorithm</li>
                    <li>Explore real-world applications like game playing and robotics</li>
                    <li>Understand the differences between RL and other ML paradigms</li>
                    <li>Recognize the power and limitations of RL</li>
                </ul>
            </div>
            <div class="japanese">
                <p>ä»Šé€±ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç’°å¢ƒã¨ç›¸äº’ä½œç”¨ã—å ±é…¬ã‚’å—ã‘å–ã‚‹ã“ã¨ã§æ„æ€æ±ºå®šã‚’å­¦ç¿’ã™ã‚‹ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªæ©Ÿæ¢°å­¦ç¿’ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã§ã‚ã‚‹å¼·åŒ–å­¦ç¿’ï¼ˆRLï¼‰ã‚’æ¢ã‚Šã¾ã™ã€‚æ•™å¸«ã‚ã‚Šå­¦ç¿’ï¼ˆãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰ã‚„æ•™å¸«ãªã—å­¦ç¿’ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¦‹ã¤ã‘ã‚‹ï¼‰ã¨ã¯ç•°ãªã‚Šã€RLã¯è©¦è¡Œã€ã‚¨ãƒ©ãƒ¼ã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’é€šã˜ã¦å­¦ç¿’ã—ã¾ã™ã€‚</p>
                <p><strong>å­¦ç¿’ç›®æ¨™:</strong></p>
                <ul>
                    <li>RLãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ç†è§£ã™ã‚‹: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€ç’°å¢ƒã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã€å ±é…¬</li>
                    <li>æ¢ç´¢ã¨æ´»ç”¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’å­¦ã¶</li>
                    <li>Qå­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç¿’å¾—ã™ã‚‹</li>
                    <li>ã‚²ãƒ¼ãƒ ãƒ—ãƒ¬ã‚¤ã‚„ãƒ­ãƒœãƒƒãƒˆå·¥å­¦ãªã©ã®å®Ÿä¸–ç•Œã®å¿œç”¨ã‚’æ¢ã‚‹</li>
                    <li>RLã¨ä»–ã®MLãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã®é•ã„ã‚’ç†è§£ã™ã‚‹</li>
                    <li>RLã®åŠ›ã¨é™ç•Œã‚’èªè­˜ã™ã‚‹</li>
                </ul>
            </div>
        </div>

        <h2>1. What is Reinforcement Learning? / å¼·åŒ–å­¦ç¿’ã¨ã¯ï¼Ÿ</h2>

        <div class="definition-box">
            <h3>Key Definition / ä¸»è¦å®šç¾©</h3>
            <p><strong>Reinforcement Learning:</strong> A machine learning paradigm where an agent learns to make sequential decisions by taking actions in an environment to maximize cumulative reward through trial and error.</p>
            <p><strong>å¼·åŒ–å­¦ç¿’:</strong> ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒè©¦è¡ŒéŒ¯èª¤ã‚’é€šã˜ã¦ç´¯ç©å ±é…¬ã‚’æœ€å¤§åŒ–ã™ã‚‹ãŸã‚ã«ç’°å¢ƒã§ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å–ã‚‹ã“ã¨ã§ã€ä¸€é€£ã®æ„æ€æ±ºå®šã‚’å­¦ç¿’ã™ã‚‹æ©Ÿæ¢°å­¦ç¿’ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã€‚</p>
        </div>

        <div class="bilingual">
            <div class="english">
                <h3>The RL Framework</h3>

                <h4>Core Components</h4>
                <div class="code-box">REINFORCEMENT LEARNING CYCLE

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚        ENVIRONMENT          â”‚
        â”‚         ç’°å¢ƒ                â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†‘           |
            Reward R_t      | State S_t
            å ±é…¬            | çŠ¶æ…‹
                |           â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚          AGENT              â”‚
        â”‚        ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    |
                Action A_t
                ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
                    â†“

The Cycle:
1. Agent observes current State (S_t)
2. Agent chooses Action (A_t)
3. Environment transitions to new State (S_t+1)
4. Environment provides Reward (R_t)
5. Agent learns from experience
6. Repeat</div>

                <h4>Detailed Components:</h4>
                <ul>
                    <li><strong>Agent (ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ):</strong> The learner/decision maker
                        <ul>
                            <li>Example: Chess player, robot, self-driving car</li>
                            <li>Goal: Learn optimal policy to maximize rewards</li>
                        </ul>
                    </li>
                    <li><strong>Environment (ç’°å¢ƒ):</strong> The world the agent interacts with
                        <ul>
                            <li>Example: Chess board, physical world, video game</li>
                            <li>Provides states and rewards</li>
                        </ul>
                    </li>
                    <li><strong>State (çŠ¶æ…‹ - S):</strong> Current situation/configuration
                        <ul>
                            <li>Example: Board position in chess, robot's location</li>
                            <li>Contains all information needed for decision</li>
                        </ul>
                    </li>
                    <li><strong>Action (ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ - A):</strong> Choices available to agent
                        <ul>
                            <li>Example: Move piece, turn left/right, accelerate</li>
                            <li>Action space can be discrete or continuous</li>
                        </ul>
                    </li>
                    <li><strong>Reward (å ±é…¬ - R):</strong> Immediate feedback signal
                        <ul>
                            <li>Positive: Good action (e.g., +1 for winning chess)</li>
                            <li>Negative: Bad action (e.g., -1 for losing, -0.01 for each step)</li>
                            <li>Zero: Neutral actions</li>
                        </ul>
                    </li>
                    <li><strong>Policy (æ–¹ç­– - Ï€):</strong> Strategy for choosing actions
                        <ul>
                            <li>Maps states to actions</li>
                            <li>What the agent learns!</li>
                            <li>Can be deterministic or probabilistic</li>
                        </ul>
                    </li>
                </ul>

                <h4>Simple Example: Robot Navigation</h4>
                <p><strong>Scenario:</strong> Robot must navigate to goal</p>
                <ul>
                    <li><strong>Agent:</strong> Robot</li>
                    <li><strong>Environment:</strong> Room with obstacles</li>
                    <li><strong>State:</strong> Robot's position (x, y)</li>
                    <li><strong>Actions:</strong> Move Up, Down, Left, Right</li>
                    <li><strong>Rewards:</strong>
                        <ul>
                            <li>+100: Reaching goal</li>
                            <li>-10: Hitting obstacle</li>
                            <li>-1: Each step (encourages efficiency)</li>
                        </ul>
                    </li>
                    <li><strong>Goal:</strong> Learn policy that reaches goal quickly while avoiding obstacles</li>
                </ul>
            </div>
            <div class="japanese">
                <h3>RLãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯</h3>

                <h4>ã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ</h4>
                <div class="code-box">å¼·åŒ–å­¦ç¿’ã‚µã‚¤ã‚¯ãƒ«

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚        ENVIRONMENT          â”‚
        â”‚         ç’°å¢ƒ                â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†‘           |
            Reward R_t      | State S_t
            å ±é…¬            | çŠ¶æ…‹
                |           â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚          AGENT              â”‚
        â”‚        ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    |
                Action A_t
                ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
                    â†“

ã‚µã‚¤ã‚¯ãƒ«:
1. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç¾åœ¨ã®çŠ¶æ…‹ã‚’è¦³å¯Ÿï¼ˆS_tï¼‰
2. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠï¼ˆA_tï¼‰
3. ç’°å¢ƒãŒæ–°ã—ã„çŠ¶æ…‹ã«é·ç§»ï¼ˆS_t+1ï¼‰
4. ç’°å¢ƒãŒå ±é…¬ã‚’æä¾›ï¼ˆR_tï¼‰
5. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒçµŒé¨“ã‹ã‚‰å­¦ç¿’
6. ç¹°ã‚Šè¿”ã—</div>

                <h4>è©³ç´°ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ:</h4>
                <ul>
                    <li><strong>ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ (Agent):</strong> å­¦ç¿’è€…/æ„æ€æ±ºå®šè€…
                        <ul>
                            <li>ä¾‹: ãƒã‚§ã‚¹ãƒ—ãƒ¬ãƒ¼ãƒ¤ãƒ¼ã€ãƒ­ãƒœãƒƒãƒˆã€è‡ªå‹•é‹è»¢è»Š</li>
                            <li>ç›®æ¨™: å ±é…¬ã‚’æœ€å¤§åŒ–ã™ã‚‹æœ€é©ãªæ–¹ç­–ã‚’å­¦ç¿’</li>
                        </ul>
                    </li>
                    <li><strong>ç’°å¢ƒ (Environment):</strong> ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç›¸äº’ä½œç”¨ã™ã‚‹ä¸–ç•Œ
                        <ul>
                            <li>ä¾‹: ãƒã‚§ã‚¹ç›¤ã€ç‰©ç†ä¸–ç•Œã€ãƒ“ãƒ‡ã‚ªã‚²ãƒ¼ãƒ </li>
                            <li>çŠ¶æ…‹ã¨å ±é…¬ã‚’æä¾›</li>
                        </ul>
                    </li>
                    <li><strong>çŠ¶æ…‹ (State - S):</strong> ç¾åœ¨ã®çŠ¶æ³/æ§‹æˆ
                        <ul>
                            <li>ä¾‹: ãƒã‚§ã‚¹ã®ç›¤é¢ä½ç½®ã€ãƒ­ãƒœãƒƒãƒˆã®ä½ç½®</li>
                            <li>æ„æ€æ±ºå®šã«å¿…è¦ãªã™ã¹ã¦ã®æƒ…å ±ã‚’å«ã‚€</li>
                        </ul>
                    </li>
                    <li><strong>ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ (Action - A):</strong> ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã‚‹é¸æŠè‚¢
                        <ul>
                            <li>ä¾‹: é§’ã‚’å‹•ã‹ã™ã€å·¦/å³ã«æ›²ãŒã‚‹ã€åŠ é€Ÿã™ã‚‹</li>
                            <li>ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç©ºé–“ã¯é›¢æ•£ã¾ãŸã¯é€£ç¶š</li>
                        </ul>
                    </li>
                    <li><strong>å ±é…¬ (Reward - R):</strong> å³æ™‚ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ä¿¡å·
                        <ul>
                            <li>æ­£: è‰¯ã„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆä¾‹: ãƒã‚§ã‚¹ã«å‹ã¤ã¨+1ï¼‰</li>
                            <li>è² : æ‚ªã„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆä¾‹: è² ã‘ã‚‹ã¨-1ã€å„ã‚¹ãƒ†ãƒƒãƒ—-0.01ï¼‰</li>
                            <li>ã‚¼ãƒ­: ä¸­ç«‹çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³</li>
                        </ul>
                    </li>
                    <li><strong>æ–¹ç­– (Policy - Ï€):</strong> ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠã™ã‚‹æˆ¦ç•¥
                        <ul>
                            <li>çŠ¶æ…‹ã‚’ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«ãƒãƒƒãƒ”ãƒ³ã‚°</li>
                            <li>ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå­¦ç¿’ã™ã‚‹ã‚‚ã®ï¼</li>
                            <li>æ±ºå®šçš„ã¾ãŸã¯ç¢ºç‡çš„</li>
                        </ul>
                    </li>
                </ul>

                <h4>ã‚·ãƒ³ãƒ—ãƒ«ãªä¾‹: ãƒ­ãƒœãƒƒãƒˆãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³</h4>
                <p><strong>ã‚·ãƒŠãƒªã‚ª:</strong> ãƒ­ãƒœãƒƒãƒˆãŒç›®æ¨™ã«ç§»å‹•ã™ã‚‹å¿…è¦ãŒã‚ã‚‹</p>
                <ul>
                    <li><strong>ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ:</strong> ãƒ­ãƒœãƒƒãƒˆ</li>
                    <li><strong>ç’°å¢ƒ:</strong> éšœå®³ç‰©ã®ã‚ã‚‹éƒ¨å±‹</li>
                    <li><strong>çŠ¶æ…‹:</strong> ãƒ­ãƒœãƒƒãƒˆã®ä½ç½®ï¼ˆxã€yï¼‰</li>
                    <li><strong>ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:</strong> ä¸Šã€ä¸‹ã€å·¦ã€å³ã«ç§»å‹•</li>
                    <li><strong>å ±é…¬:</strong>
                        <ul>
                            <li>+100: ç›®æ¨™åˆ°é”</li>
                            <li>-10: éšœå®³ç‰©ã«è¡çª</li>
                            <li>-1: å„ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆåŠ¹ç‡ã‚’ä¿ƒé€²ï¼‰</li>
                        </ul>
                    </li>
                    <li><strong>ç›®æ¨™:</strong> éšœå®³ç‰©ã‚’é¿ã‘ãªãŒã‚‰ç´ æ—©ãç›®æ¨™ã«åˆ°é”ã™ã‚‹æ–¹ç­–ã‚’å­¦ç¿’</li>
                </ul>
            </div>
        </div>

        <h2>2. Exploration vs. Exploitation / æ¢ç´¢ã¨æ´»ç”¨</h2>

        <div class="definition-box">
            <h3>The Fundamental Tradeoff</h3>
            <p><strong>Exploration:</strong> Trying new actions to discover potentially better strategies</p>
            <p><strong>æ¢ç´¢:</strong> æ½œåœ¨çš„ã«ã‚ˆã‚Šè‰¯ã„æˆ¦ç•¥ã‚’ç™ºè¦‹ã™ã‚‹ãŸã‚ã«æ–°ã—ã„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’è©¦ã™</p>
            <hr>
            <p><strong>Exploitation:</strong> Using known good actions to maximize immediate reward</p>
            <p><strong>æ´»ç”¨:</strong> å³æ™‚å ±é…¬ã‚’æœ€å¤§åŒ–ã™ã‚‹ãŸã‚ã«æ—¢çŸ¥ã®è‰¯ã„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹</p>
        </div>

        <div class="bilingual">
            <div class="english">
                <h3>Understanding the Dilemma</h3>

                <h4>The Restaurant Analogy</h4>
                <p>You're visiting a new city with many restaurants:</p>
                <ul>
                    <li><strong>Exploitation:</strong> Always go to the one good restaurant you found
                        <ul>
                            <li>Guaranteed decent meal</li>
                            <li>But might miss amazing restaurants</li>
                        </ul>
                    </li>
                    <li><strong>Exploration:</strong> Try new restaurants every time
                        <ul>
                            <li>Might discover great places</li>
                            <li>But might waste time on bad restaurants</li>
                        </ul>
                    </li>
                    <li><strong>Balanced Approach:</strong> Mostly go to known good places, occasionally try new ones
                        <ul>
                            <li>Best long-term strategy!</li>
                        </ul>
                    </li>
                </ul>

                <h4>Epsilon-Greedy Strategy (Îµ-greedy)</h4>
                <p>A simple but effective approach to balance exploration and exploitation:</p>

                <div class="code-box">EPSILON-GREEDY ALGORITHM

With probability Îµ (epsilon):
â”œâ”€ EXPLORE: Choose random action
â””â”€ Try something new

With probability (1 - Îµ):
â”œâ”€ EXPLOIT: Choose best known action
â””â”€ Use what you know works

Typical Îµ values:
â”œâ”€ Start: Îµ = 1.0 (100% exploration initially)
â”œâ”€ Decay: Gradually decrease Îµ over time
â””â”€ End: Îµ = 0.1 (10% exploration, 90% exploitation)

Example with Îµ = 0.1:
â”œâ”€ 10% of the time: Random action (explore)
â””â”€ 90% of the time: Best action (exploit)</div>

                <h4>Why This Matters</h4>
                <ul>
                    <li><strong>Too Much Exploration:</strong>
                        <ul>
                            <li>Wastes time trying bad actions</li>
                            <li>Learning is slow</li>
                            <li>Never settles on good strategy</li>
                        </ul>
                    </li>
                    <li><strong>Too Much Exploitation:</strong>
                        <ul>
                            <li>Gets stuck in local optimum</li>
                            <li>Misses better strategies</li>
                            <li>Never discovers full potential</li>
                        </ul>
                    </li>
                    <li><strong>Balanced Approach:</strong>
                        <ul>
                            <li>Explores early to find good strategies</li>
                            <li>Exploits later to maximize rewards</li>
                            <li>Achieves best long-term performance</li>
                        </ul>
                    </li>
                </ul>
            </div>
            <div class="japanese">
                <h3>ã‚¸ãƒ¬ãƒ³ãƒã®ç†è§£</h3>

                <h4>ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³ã®ä¾‹ãˆ</h4>
                <p>å¤šãã®ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³ãŒã‚ã‚‹æ–°ã—ã„éƒ½å¸‚ã‚’è¨ªã‚Œã¦ã„ã¾ã™:</p>
                <ul>
                    <li><strong>æ´»ç”¨:</strong> è¦‹ã¤ã‘ãŸ1ã¤ã®è‰¯ã„ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³ã«å¸¸ã«è¡Œã
                        <ul>
                            <li>ã¾ã¨ã‚‚ãªé£Ÿäº‹ãŒä¿è¨¼ã•ã‚Œã‚‹</li>
                            <li>ã—ã‹ã—ã€ç´ æ™´ã‚‰ã—ã„ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³ã‚’è¦‹é€ƒã™ã‹ã‚‚ã—ã‚Œãªã„</li>
                        </ul>
                    </li>
                    <li><strong>æ¢ç´¢:</strong> æ¯å›æ–°ã—ã„ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³ã‚’è©¦ã™
                        <ul>
                            <li>ç´ æ™´ã‚‰ã—ã„å ´æ‰€ã‚’ç™ºè¦‹ã™ã‚‹ã‹ã‚‚ã—ã‚Œãªã„</li>
                            <li>ã—ã‹ã—ã€æ‚ªã„ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³ã§æ™‚é–“ã‚’ç„¡é§„ã«ã™ã‚‹ã‹ã‚‚ã—ã‚Œãªã„</li>
                        </ul>
                    </li>
                    <li><strong>ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:</strong> ä¸»ã«æ—¢çŸ¥ã®è‰¯ã„å ´æ‰€ã«è¡Œãã€æ™‚ã€…æ–°ã—ã„å ´æ‰€ã‚’è©¦ã™
                        <ul>
                            <li>æœ€è‰¯ã®é•·æœŸæˆ¦ç•¥ï¼</li>
                        </ul>
                    </li>
                </ul>

                <h4>ã‚¤ãƒ—ã‚·ãƒ­ãƒ³-ã‚°ãƒªãƒ¼ãƒ‡ã‚£æˆ¦ç•¥ï¼ˆÎµ-greedyï¼‰</h4>
                <p>æ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹ã‚’ã¨ã‚‹ã‚·ãƒ³ãƒ—ãƒ«ã§åŠ¹æœçš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:</p>

                <div class="code-box">ã‚¤ãƒ—ã‚·ãƒ­ãƒ³-ã‚°ãƒªãƒ¼ãƒ‡ã‚£ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

ç¢ºç‡Îµï¼ˆã‚¤ãƒ—ã‚·ãƒ­ãƒ³ï¼‰ã§:
â”œâ”€ æ¢ç´¢: ãƒ©ãƒ³ãƒ€ãƒ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠ
â””â”€ æ–°ã—ã„ã“ã¨ã‚’è©¦ã™

ç¢ºç‡ï¼ˆ1 - Îµï¼‰ã§:
â”œâ”€ æ´»ç”¨: æœ€è‰¯ã®æ—¢çŸ¥ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠ
â””â”€ ã†ã¾ãã„ãã“ã¨ãŒã‚ã‹ã£ã¦ã„ã‚‹ã“ã¨ã‚’ä½¿ç”¨

ä¸€èˆ¬çš„ãªÎµå€¤:
â”œâ”€ é–‹å§‹: Îµ = 1.0ï¼ˆæœ€åˆã¯100%æ¢ç´¢ï¼‰
â”œâ”€ æ¸›è¡°: æ™‚é–“ã¨ã¨ã‚‚ã«Îµã‚’å¾ã€…ã«æ¸›å°‘
â””â”€ çµ‚äº†: Îµ = 0.1ï¼ˆ10%æ¢ç´¢ã€90%æ´»ç”¨ï¼‰

Îµ = 0.1ã®ä¾‹:
â”œâ”€ 10%ã®æ™‚é–“: ãƒ©ãƒ³ãƒ€ãƒ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆæ¢ç´¢ï¼‰
â””â”€ 90%ã®æ™‚é–“: æœ€è‰¯ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆæ´»ç”¨ï¼‰</div>

                <h4>ãªãœã“ã‚ŒãŒé‡è¦ã‹</h4>
                <ul>
                    <li><strong>æ¢ç´¢ãŒå¤šã™ãã‚‹:</strong>
                        <ul>
                            <li>æ‚ªã„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’è©¦ã™ã®ã«æ™‚é–“ã‚’ç„¡é§„ã«ã™ã‚‹</li>
                            <li>å­¦ç¿’ãŒé…ã„</li>
                            <li>è‰¯ã„æˆ¦ç•¥ã«è½ã¡ç€ã‹ãªã„</li>
                        </ul>
                    </li>
                    <li><strong>æ´»ç”¨ãŒå¤šã™ãã‚‹:</strong>
                        <ul>
                            <li>å±€æ‰€æœ€é©ã«é™¥ã‚‹</li>
                            <li>ã‚ˆã‚Šè‰¯ã„æˆ¦ç•¥ã‚’è¦‹é€ƒã™</li>
                            <li>å®Œå…¨ãªå¯èƒ½æ€§ã‚’ç™ºè¦‹ã—ãªã„</li>
                        </ul>
                    </li>
                    <li><strong>ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:</strong>
                        <ul>
                            <li>è‰¯ã„æˆ¦ç•¥ã‚’è¦‹ã¤ã‘ã‚‹ãŸã‚ã«æ—©æœŸã«æ¢ç´¢</li>
                            <li>å ±é…¬ã‚’æœ€å¤§åŒ–ã™ã‚‹ãŸã‚ã«å¾Œã§æ´»ç”¨</li>
                            <li>æœ€è‰¯ã®é•·æœŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆ</li>
                        </ul>
                    </li>
                </ul>
            </div>
        </div>

        <h2>3. Q-Learning Algorithm / Qå­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </h2>

        <div class="definition-box">
            <h3>What is Q-Learning? / Qå­¦ç¿’ã¨ã¯ï¼Ÿ</h3>
            <p><strong>Q-Learning:</strong> A model-free reinforcement learning algorithm that learns the quality (Q-value) of actions in each state, allowing the agent to choose optimal actions without needing a model of the environment.</p>
            <p><strong>Qå­¦ç¿’:</strong> å„çŠ¶æ…‹ã§ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®è³ªï¼ˆQå€¤ï¼‰ã‚’å­¦ç¿’ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãƒ•ãƒªãƒ¼å¼·åŒ–å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã€ç’°å¢ƒã®ãƒ¢ãƒ‡ãƒ«ã‚’å¿…è¦ã¨ã›ãšã«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒæœ€é©ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚</p>
        </div>

        <div class="bilingual">
            <div class="english">
                <h3>Understanding Q-Values</h3>

                <h4>What is a Q-Value?</h4>
                <p><strong>Q(s, a)</strong> = Expected cumulative reward of taking action 'a' in state 's' and following optimal policy thereafter</p>

                <p><strong>Q-Table:</strong> A lookup table storing Q-values for all state-action pairs</p>

                <div class="code-box">Q-TABLE EXAMPLE (Robot Navigation)

         Actions â†’
States â†“    Up    Down   Left   Right
(1,1)      2.5    -1.0   -1.0    3.2
(1,2)      5.1     2.5    0.0    1.5
(1,3)      8.3     5.1    2.5    0.0
(2,1)     -1.0     3.2    2.5    4.0
...

Higher Q-value = Better action
Agent chooses action with max Q-value (exploitation)</div>

                <h4>Q-Learning Algorithm</h4>
                <div class="code-box">Q-LEARNING PROCESS

Initialize: Set all Q(s,a) = 0

For each episode:
  1. Start in initial state s

  2. While not terminal state:

     a) Choose action using Îµ-greedy:
        - With prob Îµ: random action (explore)
        - With prob 1-Îµ: argmax Q(s,a) (exploit)

     b) Take action a, observe:
        - New state s'
        - Reward r

     c) Update Q-value:
        Q(s,a) â† Q(s,a) + Î±[r + Î³Â·max Q(s',a') - Q(s,a)]

        Where:
        Î± (alpha) = Learning rate (0-1)
        Î³ (gamma) = Discount factor (0-1)
        max Q(s',a') = Best Q-value in next state

     d) s â† s' (move to next state)

  3. Repeat until convergence</div>

                <h4>Key Parameters:</h4>
                <ul>
                    <li><strong>Learning Rate (Î± - alpha):</strong> 0 < Î± â‰¤ 1
                        <ul>
                            <li>How much to update Q-values</li>
                            <li>High Î± (0.9): Fast learning, but unstable</li>
                            <li>Low Î± (0.1): Slow learning, but stable</li>
                            <li>Typical: 0.1 - 0.5</li>
                        </ul>
                    </li>
                    <li><strong>Discount Factor (Î³ - gamma):</strong> 0 â‰¤ Î³ < 1
                        <ul>
                            <li>How much to value future rewards</li>
                            <li>Î³ = 0: Only care about immediate reward (myopic)</li>
                            <li>Î³ = 0.9: Value long-term rewards highly (far-sighted)</li>
                            <li>Î³ = 1: All future rewards equally important</li>
                            <li>Typical: 0.9 - 0.99</li>
                        </ul>
                    </li>
                    <li><strong>Exploration Rate (Îµ - epsilon):</strong> 0 â‰¤ Îµ â‰¤ 1
                        <ul>
                            <li>Probability of random exploration</li>
                            <li>Start high (1.0), decay over time (to 0.1)</li>
                        </ul>
                    </li>
                </ul>

                <h4>Update Equation Explained:</h4>
                <div class="code-box">Q(s,a) â† Q(s,a) + Î±[r + Î³Â·max Q(s',a') - Q(s,a)]
         â†‘        â†‘      â†‘  â†‘   â†‘       â†‘          â†‘
      Current   Old   Learn| |   |    Target      |
      State     Value  Rate| |   |    Value    Old Value
                Reward â”€â”€â”€â”€â”€â”˜ |   |
                Discount â”€â”€â”€â”€â”€â”€â”˜   |
                Best next Q-value â”€â”˜

Target Value = r + Î³Â·max Q(s',a')
â”œâ”€ Immediate reward (r)
â””â”€ Discounted value of best next action

TD Error = Target - Current
â””â”€ How much to adjust Q-value</div>
            </div>
            <div class="japanese">
                <h3>Qå€¤ã®ç†è§£</h3>

                <h4>Qå€¤ã¨ã¯ä½•ã‹ï¼Ÿ</h4>
                <p><strong>Q(s, a)</strong> = çŠ¶æ…‹'sã§ã‚¢ã‚¯ã‚·ãƒ§ãƒ³'a'ã‚’å–ã‚Šã€ãã®å¾Œæœ€é©ãªæ–¹ç­–ã«å¾“ã£ãŸå ´åˆã®æœŸå¾…ç´¯ç©å ±é…¬</p>

                <p><strong>Qãƒ†ãƒ¼ãƒ–ãƒ«:</strong> ã™ã¹ã¦ã®çŠ¶æ…‹-ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒšã‚¢ã®Qå€¤ã‚’ä¿å­˜ã™ã‚‹ãƒ«ãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ†ãƒ¼ãƒ–ãƒ«</p>

                <div class="code-box">Qãƒ†ãƒ¼ãƒ–ãƒ«ã®ä¾‹ï¼ˆãƒ­ãƒœãƒƒãƒˆãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰

         ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ â†’
çŠ¶æ…‹ â†“     ä¸Š     ä¸‹     å·¦     å³
(1,1)      2.5    -1.0   -1.0    3.2
(1,2)      5.1     2.5    0.0    1.5
(1,3)      8.3     5.1    2.5    0.0
(2,1)     -1.0     3.2    2.5    4.0
...

Qå€¤ãŒé«˜ã„ = ã‚ˆã‚Šè‰¯ã„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯æœ€å¤§Qå€¤ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠï¼ˆæ´»ç”¨ï¼‰</div>

                <h4>Qå­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ </h4>
                <div class="code-box">Qå­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹

åˆæœŸåŒ–: ã™ã¹ã¦ã®Q(s,a) = 0ã«è¨­å®š

å„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã«å¯¾ã—ã¦:
  1. åˆæœŸçŠ¶æ…‹sã‹ã‚‰é–‹å§‹

  2. çµ‚ç«¯çŠ¶æ…‹ã§ãªã„é™ã‚Š:

     a) Îµ-greedyã‚’ä½¿ç”¨ã—ã¦ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠ:
        - ç¢ºç‡Îµ: ãƒ©ãƒ³ãƒ€ãƒ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆæ¢ç´¢ï¼‰
        - ç¢ºç‡1-Îµ: argmax Q(s,a)ï¼ˆæ´»ç”¨ï¼‰

     b) ã‚¢ã‚¯ã‚·ãƒ§ãƒ³aã‚’å–ã‚Šã€è¦³å¯Ÿ:
        - æ–°ã—ã„çŠ¶æ…‹s'
        - å ±é…¬r

     c) Qå€¤ã‚’æ›´æ–°:
        Q(s,a) â† Q(s,a) + Î±[r + Î³Â·max Q(s',a') - Q(s,a)]

        ã“ã“ã§:
        Î± (ã‚¢ãƒ«ãƒ•ã‚¡) = å­¦ç¿’ç‡ï¼ˆ0-1ï¼‰
        Î³ (ã‚¬ãƒ³ãƒ) = å‰²å¼•ç‡ï¼ˆ0-1ï¼‰
        max Q(s',a') = æ¬¡çŠ¶æ…‹ã§ã®æœ€è‰¯ã®Qå€¤

     d) s â† s'ï¼ˆæ¬¡ã®çŠ¶æ…‹ã«ç§»å‹•ï¼‰

  3. åæŸã™ã‚‹ã¾ã§ç¹°ã‚Šè¿”ã™</div>

                <h4>ä¸»è¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:</h4>
                <ul>
                    <li><strong>å­¦ç¿’ç‡ï¼ˆÎ± - ã‚¢ãƒ«ãƒ•ã‚¡ï¼‰:</strong> 0 < Î± â‰¤ 1
                        <ul>
                            <li>Qå€¤ã‚’ã©ã‚Œã ã‘æ›´æ–°ã™ã‚‹ã‹</li>
                            <li>é«˜Î±ï¼ˆ0.9ï¼‰: é€Ÿã„å­¦ç¿’ã€ã—ã‹ã—ä¸å®‰å®š</li>
                            <li>ä½Î±ï¼ˆ0.1ï¼‰: é…ã„å­¦ç¿’ã€ã—ã‹ã—å®‰å®š</li>
                            <li>ä¸€èˆ¬çš„: 0.1 - 0.5</li>
                        </ul>
                    </li>
                    <li><strong>å‰²å¼•ç‡ï¼ˆÎ³ - ã‚¬ãƒ³ãƒï¼‰:</strong> 0 â‰¤ Î³ < 1
                        <ul>
                            <li>å°†æ¥ã®å ±é…¬ã‚’ã©ã‚Œã ã‘è©•ä¾¡ã™ã‚‹ã‹</li>
                            <li>Î³ = 0: å³æ™‚å ±é…¬ã®ã¿ã‚’æ°—ã«ã™ã‚‹ï¼ˆè¿‘è¦–çœ¼çš„ï¼‰</li>
                            <li>Î³ = 0.9: é•·æœŸå ±é…¬ã‚’é«˜ãè©•ä¾¡ï¼ˆé è¦–ï¼‰</li>
                            <li>Î³ = 1: ã™ã¹ã¦ã®å°†æ¥å ±é…¬ãŒåŒç­‰ã«é‡è¦</li>
                            <li>ä¸€èˆ¬çš„: 0.9 - 0.99</li>
                        </ul>
                    </li>
                    <li><strong>æ¢ç´¢ç‡ï¼ˆÎµ - ã‚¤ãƒ—ã‚·ãƒ­ãƒ³ï¼‰:</strong> 0 â‰¤ Îµ â‰¤ 1
                        <ul>
                            <li>ãƒ©ãƒ³ãƒ€ãƒ æ¢ç´¢ã®ç¢ºç‡</li>
                            <li>é«˜ãã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆ1.0ï¼‰ã€æ™‚é–“ã¨ã¨ã‚‚ã«æ¸›è¡°ï¼ˆ0.1ã¾ã§ï¼‰</li>
                        </ul>
                    </li>
                </ul>

                <h4>æ›´æ–°å¼ã®èª¬æ˜:</h4>
                <div class="code-box">Q(s,a) â† Q(s,a) + Î±[r + Î³Â·max Q(s',a') - Q(s,a)]
         â†‘        â†‘      â†‘  â†‘   â†‘       â†‘          â†‘
      ç¾åœ¨     å¤ã„   å­¦ç¿’| |   |    ç›®æ¨™        |
      çŠ¶æ…‹     å€¤     ç‡  | |   |    å€¤       å¤ã„å€¤
                å ±é…¬ â”€â”€â”€â”€â”€â”€â”˜ |   |
                å‰²å¼• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   |
                æœ€è‰¯ã®æ¬¡Qå€¤ â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ç›®æ¨™å€¤ = r + Î³Â·max Q(s',a')
â”œâ”€ å³æ™‚å ±é…¬ï¼ˆrï¼‰
â””â”€ å‰²å¼•ã•ã‚ŒãŸæœ€è‰¯ã®æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®å€¤

TDèª¤å·® = ç›®æ¨™ - ç¾åœ¨
â””â”€ Qå€¤ã‚’ã©ã‚Œã ã‘èª¿æ•´ã™ã‚‹ã‹</div>
            </div>
        </div>

        <h2>4. Deep Q-Networks (DQN) / æ·±å±¤Qãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯</h2>

        <div class="bilingual">
            <div class="english">
                <h3>Scaling Q-Learning with Neural Networks</h3>

                <h4>The Problem with Q-Tables</h4>
                <p>Q-tables work well for small problems, but fail for complex environments:</p>
                <ul>
                    <li><strong>Chess:</strong> ~10^47 possible states - can't store in table!</li>
                    <li><strong>Atari Games:</strong> Millions of possible screen configurations</li>
                    <li><strong>Continuous States:</strong> Infinite states (e.g., robot arm angles)</li>
                </ul>

                <h4>Solution: Deep Q-Network (DQN)</h4>
                <p>Instead of storing Q-values in a table, use a neural network to <strong>approximate</strong> Q-values!</p>

                <div class="code-box">DQN ARCHITECTURE

Input: State (e.g., game screen pixels)
  â†“
[Convolutional Layers]  â† Process visual input
  â†“
[Dense Layers]          â† Learn representations
  â†“
Output: Q-values for each action
  [Q(s, aâ‚), Q(s, aâ‚‚), Q(s, aâ‚ƒ), ...]

Example - Atari Breakout:
Input: 84Ã—84Ã—4 grayscale frames (recent history)
Output: [Q(s, left), Q(s, right), Q(s, fire)]
Action: Choose argmax (highest Q-value)</div>

                <h4>DQN Innovations</h4>
                <ul>
                    <li><strong>Experience Replay:</strong>
                        <ul>
                            <li>Store past experiences (s, a, r, s') in memory buffer</li>
                            <li>Sample random batches for training</li>
                            <li>Breaks correlation between consecutive samples</li>
                            <li>More efficient learning</li>
                        </ul>
                    </li>
                    <li><strong>Target Network:</strong>
                        <ul>
                            <li>Use separate network for calculating target Q-values</li>
                            <li>Update target network periodically (not every step)</li>
                            <li>Stabilizes training</li>
                        </ul>
                    </li>
                </ul>

                <h4>DQN Success: Atari Games</h4>
                <p>DeepMind's DQN (2015) learned to play 49 Atari games from raw pixels:</p>
                <ul>
                    <li>No game-specific features or rules</li>
                    <li>Same architecture for all games</li>
                    <li>Achieved human-level or superhuman performance on many games</li>
                    <li>Major breakthrough demonstrating RL can learn complex behaviors from high-dimensional inputs</li>
                </ul>
            </div>
            <div class="japanese">
                <h3>ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚‹Qå­¦ç¿’ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°</h3>

                <h4>Qãƒ†ãƒ¼ãƒ–ãƒ«ã®å•é¡Œ</h4>
                <p>Qãƒ†ãƒ¼ãƒ–ãƒ«ã¯å°ã•ãªå•é¡Œã«ã¯ã†ã¾ãæ©Ÿèƒ½ã—ã¾ã™ãŒã€è¤‡é›‘ãªç’°å¢ƒã§ã¯å¤±æ•—ã—ã¾ã™:</p>
                <ul>
                    <li><strong>ãƒã‚§ã‚¹:</strong> ~10^47ã®å¯èƒ½ãªçŠ¶æ…‹ - ãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜ã§ããªã„ï¼</li>
                    <li><strong>Atariã‚²ãƒ¼ãƒ :</strong> æ•°ç™¾ä¸‡ã®å¯èƒ½ãªç”»é¢æ§‹æˆ</li>
                    <li><strong>é€£ç¶šçŠ¶æ…‹:</strong> ç„¡é™ã®çŠ¶æ…‹ï¼ˆä¾‹: ãƒ­ãƒœãƒƒãƒˆã‚¢ãƒ¼ãƒ ã®è§’åº¦ï¼‰</li>
                </ul>

                <h4>è§£æ±ºç­–: æ·±å±¤Qãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆDQNï¼‰</h4>
                <p>Qãƒ†ãƒ¼ãƒ–ãƒ«ã«å€¤ã‚’ä¿å­˜ã™ã‚‹ä»£ã‚ã‚Šã«ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ã¦Qå€¤ã‚’<strong>è¿‘ä¼¼</strong>ã—ã¾ã™ï¼</p>

                <div class="code-box">DQNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

å…¥åŠ›: çŠ¶æ…‹ï¼ˆä¾‹: ã‚²ãƒ¼ãƒ ç”»é¢ãƒ”ã‚¯ã‚»ãƒ«ï¼‰
  â†“
[ç•³ã¿è¾¼ã¿å±¤]          â† è¦–è¦šå…¥åŠ›ã‚’å‡¦ç†
  â†“
[å…¨çµåˆå±¤]            â† è¡¨ç¾ã‚’å­¦ç¿’
  â†“
å‡ºåŠ›: å„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®Qå€¤
  [Q(s, aâ‚), Q(s, aâ‚‚), Q(s, aâ‚ƒ), ...]

ä¾‹ - Atariãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆ:
å…¥åŠ›: 84Ã—84Ã—4ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆæœ€è¿‘ã®å±¥æ­´ï¼‰
å‡ºåŠ›: [Q(s, å·¦), Q(s, å³), Q(s, ç™ºå°„)]
ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: argmaxã‚’é¸æŠï¼ˆæœ€é«˜Qå€¤ï¼‰</div>

                <h4>DQNã®é©æ–°</h4>
                <ul>
                    <li><strong>çµŒé¨“ãƒªãƒ—ãƒ¬ã‚¤:</strong>
                        <ul>
                            <li>éå»ã®çµŒé¨“ï¼ˆsã€aã€rã€s'ï¼‰ã‚’ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ•ã‚¡ã«ä¿å­˜</li>
                            <li>è¨“ç·´ã®ãŸã‚ã«ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒƒãƒã‚’ã‚µãƒ³ãƒ—ãƒ«</li>
                            <li>é€£ç¶šã‚µãƒ³ãƒ—ãƒ«é–“ã®ç›¸é–¢ã‚’å£Šã™</li>
                            <li>ã‚ˆã‚ŠåŠ¹ç‡çš„ãªå­¦ç¿’</li>
                        </ul>
                    </li>
                    <li><strong>ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯:</strong>
                        <ul>
                            <li>ç›®æ¨™Qå€¤ã®è¨ˆç®—ã«åˆ¥ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨</li>
                            <li>ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å®šæœŸçš„ã«æ›´æ–°ï¼ˆæ¯ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ãªã„ï¼‰</li>
                            <li>è¨“ç·´ã‚’å®‰å®šåŒ–</li>
                        </ul>
                    </li>
                </ul>

                <h4>DQNã®æˆåŠŸ: Atariã‚²ãƒ¼ãƒ </h4>
                <p>DeepMindã®DQNï¼ˆ2015å¹´ï¼‰ã¯ç”Ÿã®ãƒ”ã‚¯ã‚»ãƒ«ã‹ã‚‰49ã®Atariã‚²ãƒ¼ãƒ ã‚’ãƒ—ãƒ¬ã‚¤ã™ã‚‹ã“ã¨ã‚’å­¦ç¿’ã—ã¾ã—ãŸ:</p>
                <ul>
                    <li>ã‚²ãƒ¼ãƒ å›ºæœ‰ã®ç‰¹å¾´ã‚„ãƒ«ãƒ¼ãƒ«ãªã—</li>
                    <li>ã™ã¹ã¦ã®ã‚²ãƒ¼ãƒ ã§åŒã˜ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£</li>
                    <li>å¤šãã®ã‚²ãƒ¼ãƒ ã§äººé–“ãƒ¬ãƒ™ãƒ«ã¾ãŸã¯è¶…äººçš„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆ</li>
                    <li>RLãŒé«˜æ¬¡å…ƒå…¥åŠ›ã‹ã‚‰è¤‡é›‘ãªè¡Œå‹•ã‚’å­¦ç¿’ã§ãã‚‹ã“ã¨ã‚’ç¤ºã™ä¸»è¦ãªãƒ–ãƒ¬ãƒ¼ã‚¯ã‚¹ãƒ«ãƒ¼</li>
                </ul>
            </div>
        </div>

        <h2>5. Real-World Applications / å®Ÿä¸–ç•Œã®å¿œç”¨</h2>

        <div class="highlight-box">
            <h3>DeepMind AlphaGo: Mastering Go</h3>
            <h4>Link: <a href="https://deepmind.google/technologies/alphago/" target="_blank">https://deepmind.google/technologies/alphago/</a></h4>

            <div class="bilingual">
                <div class="english">
                    <p><strong>Challenge:</strong> Master the ancient game of Go, considered much harder than chess due to its enormous complexity.</p>

                    <p><strong>Why Go is Hard:</strong></p>
                    <ul>
                        <li><strong>State Space:</strong> 10^170 possible board positions (more than atoms in universe!)</li>
                        <li><strong>Branching Factor:</strong> ~250 legal moves per turn (vs ~35 in chess)</li>
                        <li><strong>Intuition Required:</strong> Evaluation requires deep pattern recognition, not just calculation</li>
                        <li><strong>Long-term Planning:</strong> Games last 200+ moves with subtle strategies</li>
                    </ul>

                    <p><strong>AlphaGo's RL Approach:</strong></p>
                    <ul>
                        <li><strong>Policy Network:</strong> Neural network that suggests good moves
                            <ul>
                                <li>Trained on expert human games (supervised learning)</li>
                                <li>Further improved through self-play (reinforcement learning)</li>
                            </ul>
                        </li>
                        <li><strong>Value Network:</strong> Evaluates board positions
                            <ul>
                                <li>Predicts winner from current position</li>
                                <li>Trained through self-play</li>
                            </ul>
                        </li>
                        <li><strong>Monte Carlo Tree Search (MCTS):</strong> Planning algorithm
                            <ul>
                                <li>Simulates thousands of possible game sequences</li>
                                <li>Guided by policy and value networks</li>
                                <li>Balances exploration and exploitation</li>
                            </ul>
                        </li>
                    </ul>

                    <p><strong>Training Process:</strong></p>
                    <ol>
                        <li><strong>Supervised Learning:</strong> Learn from 30 million expert moves</li>
                        <li><strong>Self-Play Reinforcement Learning:</strong>
                            <ul>
                                <li>Play millions of games against itself</li>
                                <li>Reward: +1 for winning, -1 for losing</li>
                                <li>Gradually improve through trial and error</li>
                            </ul>
                        </li>
                        <li><strong>Policy Improvement:</strong> Each generation plays previous versions
                            <ul>
                                <li>Only keep versions that win consistently</li>
                                <li>Ensures continuous improvement</li>
                            </ul>
                        </li>
                    </ol>

                    <p><strong>Historic Achievement:</strong></p>
                    <ul>
                        <li><strong>March 2016:</strong> AlphaGo defeated Lee Sedol 4-1
                            <ul>
                                <li>Lee Sedol: World champion, 9-dan professional</li>
                                <li>First time AI beat world champion at Go</li>
                                <li>Move 37 in Game 2: Completely novel, beautiful move</li>
                            </ul>
                        </li>
                        <li><strong>Impact:</strong>
                            <ul>
                                <li>Proved RL can master highly complex strategic domains</li>
                                <li>Discovered new strategies unknown to humans</li>
                                <li>Changed how professionals play Go</li>
                            </ul>
                        </li>
                    </ul>

                    <p><strong>AlphaGo Zero (2017):</strong></p>
                    <ul>
                        <li>Learned purely from self-play (no human games!)</li>
                        <li>Started with only the rules of Go</li>
                        <li>Defeated original AlphaGo 100-0</li>
                        <li>Trained in only 3 days</li>
                        <li>Demonstrates power of pure reinforcement learning</li>
                    </ul>
                </div>
                <div class="japanese">
                    <p><strong>èª²é¡Œ:</strong> è†¨å¤§ãªè¤‡é›‘ã•ã®ãŸã‚ãƒã‚§ã‚¹ã‚ˆã‚Šã‚‚ã¯ã‚‹ã‹ã«å›°é›£ã¨è€ƒãˆã‚‰ã‚Œã‚‹å¤ä»£ã®ã‚²ãƒ¼ãƒ ã€å›²ç¢ã‚’ãƒã‚¹ã‚¿ãƒ¼ã™ã‚‹ã€‚</p>

                    <p><strong>ãªãœå›²ç¢ã¯é›£ã—ã„ã®ã‹:</strong></p>
                    <ul>
                        <li><strong>çŠ¶æ…‹ç©ºé–“:</strong> 10^170ã®å¯èƒ½ãªç›¤é¢ä½ç½®ï¼ˆå®‡å®™ã®åŸå­ã‚ˆã‚Šå¤šã„ï¼ï¼‰</li>
                        <li><strong>åˆ†å²å› å­:</strong> ã‚¿ãƒ¼ãƒ³ã”ã¨ã«ç´„250ã®åˆæ³•æ‰‹ï¼ˆãƒã‚§ã‚¹ã¯ç´„35ï¼‰</li>
                        <li><strong>ç›´æ„ŸãŒå¿…è¦:</strong> è©•ä¾¡ã«ã¯å˜ãªã‚‹è¨ˆç®—ã§ã¯ãªãæ·±ã„ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ãŒå¿…è¦</li>
                        <li><strong>é•·æœŸè¨ˆç”»:</strong> ã‚²ãƒ¼ãƒ ã¯å¾®å¦™ãªæˆ¦ç•¥ã§200ä»¥ä¸Šã®æ‰‹ãŒç¶šã</li>
                    </ul>

                    <p><strong>AlphaGoã®RLã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:</strong></p>
                    <ul>
                        <li><strong>æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯:</strong> è‰¯ã„æ‰‹ã‚’ææ¡ˆã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
                            <ul>
                                <li>ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆäººé–“ã‚²ãƒ¼ãƒ ã§è¨“ç·´ï¼ˆæ•™å¸«ã‚ã‚Šå­¦ç¿’ï¼‰</li>
                                <li>è‡ªå·±å¯¾æˆ¦ã‚’é€šã˜ã¦ã•ã‚‰ã«æ”¹å–„ï¼ˆå¼·åŒ–å­¦ç¿’ï¼‰</li>
                            </ul>
                        </li>
                        <li><strong>ä¾¡å€¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯:</strong> ç›¤é¢ä½ç½®ã‚’è©•ä¾¡
                            <ul>
                                <li>ç¾åœ¨ä½ç½®ã‹ã‚‰å‹è€…ã‚’äºˆæ¸¬</li>
                                <li>è‡ªå·±å¯¾æˆ¦ã‚’é€šã˜ã¦è¨“ç·´</li>
                            </ul>
                        </li>
                        <li><strong>ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æœ¨æ¢ç´¢ï¼ˆMCTSï¼‰:</strong> è¨ˆç”»ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
                            <ul>
                                <li>æ•°åƒã®å¯èƒ½ãªã‚²ãƒ¼ãƒ ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ</li>
                                <li>æ–¹ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ä¾¡å€¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«å°ã‹ã‚Œã‚‹</li>
                                <li>æ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹</li>
                            </ul>
                        </li>
                    </ul>

                    <p><strong>è¨“ç·´ãƒ—ãƒ­ã‚»ã‚¹:</strong></p>
                    <ol>
                        <li><strong>æ•™å¸«ã‚ã‚Šå­¦ç¿’:</strong> 3000ä¸‡ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆæ‰‹ã‹ã‚‰å­¦ç¿’</li>
                        <li><strong>è‡ªå·±å¯¾æˆ¦å¼·åŒ–å­¦ç¿’:</strong>
                            <ul>
                                <li>è‡ªåˆ†è‡ªèº«ã¨æ•°ç™¾ä¸‡ã‚²ãƒ¼ãƒ ã‚’ãƒ—ãƒ¬ã‚¤</li>
                                <li>å ±é…¬: å‹ã¡ã§+1ã€è² ã‘ã§-1</li>
                                <li>è©¦è¡ŒéŒ¯èª¤ã‚’é€šã˜ã¦å¾ã€…ã«æ”¹å–„</li>
                            </ul>
                        </li>
                        <li><strong>æ–¹ç­–æ”¹å–„:</strong> å„ä¸–ä»£ãŒä»¥å‰ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨å¯¾æˆ¦
                            <ul>
                                <li>ä¸€è²«ã—ã¦å‹ã¤ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ã¿ã‚’ä¿æŒ</li>
                                <li>ç¶™ç¶šçš„æ”¹å–„ã‚’ç¢ºä¿</li>
                            </ul>
                        </li>
                    </ol>

                    <p><strong>æ­´å²çš„æˆæœ:</strong></p>
                    <ul>
                        <li><strong>2016å¹´3æœˆ:</strong> AlphaGoãŒã‚¤ãƒ»ã‚»ãƒ‰ãƒ«ã‚’4-1ã§ç ´ã‚‹
                            <ul>
                                <li>ã‚¤ãƒ»ã‚»ãƒ‰ãƒ«: ä¸–ç•Œãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³ã€9æ®µãƒ—ãƒ­</li>
                                <li>AIãŒå›²ç¢ã§ä¸–ç•Œãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³ã«å‹ã£ãŸåˆã‚ã¦</li>
                                <li>ç¬¬2å±€ã®37æ‰‹ç›®: å®Œå…¨ã«æ–°ã—ã„ã€ç¾ã—ã„æ‰‹</li>
                            </ul>
                        </li>
                        <li><strong>å½±éŸ¿:</strong>
                            <ul>
                                <li>RLãŒéå¸¸ã«è¤‡é›‘ãªæˆ¦ç•¥çš„é ˜åŸŸã‚’ãƒã‚¹ã‚¿ãƒ¼ã§ãã‚‹ã“ã¨ã‚’è¨¼æ˜</li>
                                <li>äººé–“ã«çŸ¥ã‚‰ã‚Œã¦ã„ãªã„æ–°ã—ã„æˆ¦ç•¥ã‚’ç™ºè¦‹</li>
                                <li>ãƒ—ãƒ­ãŒå›²ç¢ã‚’ãƒ—ãƒ¬ã‚¤ã™ã‚‹æ–¹æ³•ã‚’å¤‰ãˆãŸ</li>
                            </ul>
                        </li>
                    </ul>

                    <p><strong>AlphaGo Zeroï¼ˆ2017å¹´ï¼‰:</strong></p>
                    <ul>
                        <li>ç´”ç²‹ã«è‡ªå·±å¯¾æˆ¦ã‹ã‚‰å­¦ç¿’ï¼ˆäººé–“ã®ã‚²ãƒ¼ãƒ ãªã—ï¼ï¼‰</li>
                        <li>å›²ç¢ã®ãƒ«ãƒ¼ãƒ«ã ã‘ã§é–‹å§‹</li>
                        <li>ã‚ªãƒªã‚¸ãƒŠãƒ«ã®AlphaGoã‚’100-0ã§ç ´ã‚‹</li>
                        <li>ã‚ãšã‹3æ—¥ã§è¨“ç·´</li>
                        <li>ç´”ç²‹ãªå¼·åŒ–å­¦ç¿’ã®åŠ›ã‚’å®Ÿè¨¼</li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="study-tip">
            <h3>Study Tips for Reinforcement Learning</h3>
            <ul>
                <li>Draw the RL cycle diagram (Agent â†” Environment) from memory</li>
                <li>Understand the exploration-exploitation tradeoff with examples</li>
                <li>Practice the Q-learning update equation step-by-step</li>
                <li>Know the difference between Q-learning (tables) and DQN (neural networks)</li>
                <li>Understand why AlphaGo needed both supervised and reinforcement learning</li>
                <li>Be able to explain when RL is better than supervised learning</li>
                <li>Memorize key parameters: Î± (learning rate), Î³ (discount), Îµ (exploration)</li>
            </ul>
        </div>

        <h2>ğŸ“ Test Questions / ãƒ†ã‚¹ãƒˆå•é¡Œ</h2>

        <div class="test-question">
            <h4>TEST QUESTION 1: Multiple Choice</h4>
            <p><strong>What is the main difference between reinforcement learning and supervised learning?</strong></p>
            <ol type="A">
                <li>RL uses more data than supervised learning</li>
                <li>RL learns from rewards through trial and error, supervised learning uses labeled examples</li>
                <li>RL is faster to train</li>
                <li>RL can only work with images</li>
            </ol>
            <p><strong>Answer:</strong> B - RL learns by interacting with an environment and receiving rewards, while supervised learning requires labeled training data.</p>
        </div>

        <div class="test-question">
            <h4>TEST QUESTION 2: Multiple Choice</h4>
            <p><strong>In the exploration-exploitation tradeoff, what does "exploitation" mean?</strong></p>
            <ol type="A">
                <li>Trying new random actions to discover better strategies</li>
                <li>Using known good actions to maximize immediate reward</li>
                <li>Punishing bad actions</li>
                <li>Sharing knowledge with other agents</li>
            </ol>
            <p><strong>Answer:</strong> B - Exploitation means using what you already know works well to maximize rewards.</p>
        </div>

        <div class="test-question">
            <h4>TEST QUESTION 3: Multiple Choice</h4>
            <p><strong>In Q-learning, what does the discount factor Î³ (gamma) control?</strong></p>
            <ol type="A">
                <li>How fast the agent learns</li>
                <li>How much the agent values future rewards vs immediate rewards</li>
                <li>How often the agent explores</li>
                <li>The number of training episodes</li>
            </ol>
            <p><strong>Answer:</strong> B - Gamma determines how much future rewards are valued. High gamma = value long-term rewards, low gamma = focus on immediate rewards.</p>
        </div>

        <div class="test-question">
            <h4>TEST QUESTION 4: Multiple Choice</h4>
            <p><strong>Why did DeepMind develop Deep Q-Networks (DQN) instead of using traditional Q-tables?</strong></p>
            <ol type="A">
                <li>Q-tables are too slow</li>
                <li>Q-tables can't handle large or continuous state spaces</li>
                <li>Q-tables require too much training data</li>
                <li>Q-tables don't work for games</li>
            </ol>
            <p><strong>Answer:</strong> B - Q-tables become impractical for complex environments with millions of states or continuous state spaces.</p>
        </div>

        <div class="test-question">
            <h4>TEST QUESTION 5: Multiple Choice</h4>
            <p><strong>What was revolutionary about AlphaGo Zero compared to the original AlphaGo?</strong></p>
            <ol type="A">
                <li>It learned purely from self-play without any human game data</li>
                <li>It used a larger neural network</li>
                <li>It trained for a longer time</li>
                <li>It could play multiple games</li>
            </ol>
            <p><strong>Answer:</strong> A - AlphaGo Zero started only with the rules of Go and learned entirely through self-play, proving pure RL can surpass human knowledge.</p>
        </div>

        <div class="test-question">
            <h4>TEST QUESTION 6: Short Answer</h4>
            <p><strong>Explain the complete reinforcement learning cycle including all components: agent, environment, state, action, and reward.</strong></p>
            <p><strong>Sample Answer:</strong></p>
            <p>The RL cycle works as follows:</p>
            <ol>
                <li><strong>Agent observes State:</strong> The agent receives information about the current situation from the environment (e.g., robot's position, game board configuration)</li>
                <li><strong>Agent chooses Action:</strong> Based on its current policy, the agent selects an action to take (e.g., move left, make a chess move)</li>
                <li><strong>Environment transitions:</strong> The environment changes to a new state based on the action taken</li>
                <li><strong>Agent receives Reward:</strong> The environment provides feedback - positive reward for good actions, negative for bad, or zero for neutral</li>
                <li><strong>Agent learns:</strong> The agent updates its policy based on the experience (state, action, reward, new state)</li>
                <li><strong>Repeat:</strong> The cycle continues until a terminal state (goal reached or episode ends)</li>
            </ol>
            <p>Over many iterations, the agent learns which actions lead to high cumulative rewards and develops an optimal policy.</p>
        </div>

        <div class="test-question">
            <h4>TEST QUESTION 7: Short Answer</h4>
            <p><strong>Describe the Îµ-greedy strategy for balancing exploration and exploitation. Include typical epsilon values and how they change over time.</strong></p>
            <p><strong>Sample Answer:</strong></p>
            <p>Îµ-greedy is a strategy that balances trying new things (exploration) with using known good actions (exploitation):</p>
            <p><strong>How it works:</strong></p>
            <ul>
                <li>With probability Îµ (epsilon): Choose a random action (explore)</li>
                <li>With probability (1-Îµ): Choose the action with highest Q-value (exploit)</li>
            </ul>
            <p><strong>Epsilon decay schedule:</strong></p>
            <ul>
                <li><strong>Start:</strong> Îµ = 1.0 (100% exploration) - agent knows nothing, so must explore</li>
                <li><strong>During training:</strong> Gradually decrease Îµ (e.g., multiply by 0.995 each episode)</li>
                <li><strong>End:</strong> Îµ = 0.1 (10% exploration, 90% exploitation) - agent mostly uses what it learned but still explores occasionally</li>
            </ul>
            <p><strong>Why this works:</strong> Early exploration discovers good strategies. Later exploitation maximizes rewards using learned knowledge while occasional exploration prevents getting stuck in local optima.</p>
        </div>

        <div class="test-question">
            <h4>TEST QUESTION 8: Short Answer</h4>
            <p><strong>Explain the Q-learning update equation. What does each component represent?</strong></p>
            <p><strong>Sample Answer:</strong></p>
            <p><strong>Q-learning Update Equation:</strong></p>
            <p>Q(s,a) â† Q(s,a) + Î±[r + Î³Â·max Q(s',a') - Q(s,a)]</p>
            <p><strong>Components:</strong></p>
            <ul>
                <li><strong>Q(s,a):</strong> Current Q-value for taking action 'a' in state 's' (what we're updating)</li>
                <li><strong>Î± (alpha):</strong> Learning rate (0-1) - how much to adjust Q-value (typically 0.1-0.5)</li>
                <li><strong>r:</strong> Immediate reward received after taking action</li>
                <li><strong>Î³ (gamma):</strong> Discount factor (0-1) - how much to value future rewards (typically 0.9-0.99)</li>
                <li><strong>max Q(s',a'):</strong> Best Q-value possible in the new state s'</li>
                <li><strong>[r + Î³Â·max Q(s',a')]:</strong> Target value - what Q(s,a) should be</li>
                <li><strong>[r + Î³Â·max Q(s',a') - Q(s,a)]:</strong> TD Error - difference between target and current</li>
            </ul>
            <p><strong>Intuition:</strong> Update current Q-value by moving it toward the target value (immediate reward + discounted best future value), with step size controlled by learning rate.</p>
        </div>

        <div class="test-question">
            <h4>TEST QUESTION 9: Application Question</h4>
            <p><strong>AlphaGo used reinforcement learning to master Go. Explain:</strong></p>
            <p><strong>a) Why Go is much harder than chess for AI</strong></p>
            <p><strong>b) How AlphaGo combined supervised and reinforcement learning</strong></p>
            <p><strong>c) What AlphaGo Zero demonstrated about pure reinforcement learning</strong></p>
            <p><strong>Sample Answer:</strong></p>
            <p><strong>a) Why Go is Harder:</strong></p>
            <ul>
                <li><strong>State Space:</strong> 10^170 positions vs chess's 10^47 - impossibly large for brute force</li>
                <li><strong>Branching Factor:</strong> ~250 legal moves per turn vs ~35 in chess - can't search all possibilities</li>
                <li><strong>Evaluation Difficulty:</strong> Requires pattern recognition and intuition, not just material counting</li>
                <li><strong>Long-term Strategy:</strong> Subtle moves pay off 100+ moves later</li>
            </ul>
            <p><strong>b) AlphaGo's Hybrid Approach:</strong></p>
            <ol>
                <li><strong>Supervised Learning Phase:</strong> Train policy network on 30 million expert human moves to learn good move patterns</li>
                <li><strong>Reinforcement Learning Phase:</strong> Improve through millions of self-play games, receiving +1 reward for winning, -1 for losing</li>
                <li><strong>Combined:</strong> Use supervised learning for initial strategy, then RL to discover better strategies beyond human knowledge</li>
            </ol>
            <p><strong>c) AlphaGo Zero's Achievement:</strong></p>
            <ul>
                <li>Learned purely from self-play starting only with Go rules - no human games!</li>
                <li>Defeated original AlphaGo 100-0 in just 3 days of training</li>
                <li>Proved pure RL can discover superhuman strategies without human knowledge</li>
                <li>Demonstrated RL's potential to exceed human expertise in complex domains</li>
            </ul>
        </div>

        <div class="test-question">
            <h4>TEST QUESTION 10: Essay Question</h4>
            <p><strong>Compare the three main machine learning paradigms: supervised learning, unsupervised learning, and reinforcement learning. For each:</strong></p>
            <ul>
                <li>Explain how it learns</li>
                <li>What kind of data/feedback it requires</li>
                <li>Give a real-world example</li>
                <li>Describe one advantage and one limitation</li>
                <li>Explain when you would choose this paradigm</li>
            </ul>
            <p><strong>Sample Answer:</strong></p>

            <p><strong>Supervised Learning:</strong></p>
            <p><strong>How it learns:</strong> Learns from labeled examples, finding patterns that map inputs to correct outputs</p>
            <p><strong>Data required:</strong> Labeled training data - each example has input features and correct output label</p>
            <p><strong>Example:</strong> Email spam detection - emails labeled as spam/not spam, model learns to classify new emails</p>
            <p><strong>Advantage:</strong> High accuracy when sufficient labeled data available; clear evaluation metrics</p>
            <p><strong>Limitation:</strong> Requires expensive, time-consuming manual labeling; can't discover novel solutions beyond training data</p>
            <p><strong>When to use:</strong> When you have labeled data and a clear prediction task with known correct answers</p>

            <p><strong>Unsupervised Learning:</strong></p>
            <p><strong>How it learns:</strong> Discovers hidden patterns and structures in data without labels</p>
            <p><strong>Data required:</strong> Unlabeled data - just raw inputs, no correct outputs</p>
            <p><strong>Example:</strong> Customer segmentation - group customers by behavior without predefined categories</p>
            <p><strong>Advantage:</strong> No labeling required; can discover unexpected patterns humans might miss</p>
            <p><strong>Limitation:</strong> Evaluation is subjective; results may be difficult to interpret or validate</p>
            <p><strong>When to use:</strong> When exploring data to find natural groupings or when labels are unavailable</p>

            <p><strong>Reinforcement Learning:</strong></p>
            <p><strong>How it learns:</strong> Learns through trial and error by interacting with environment and receiving rewards</p>
            <p><strong>Data required:</strong> No explicit dataset - generates own experience through environmental interaction; needs reward signal</p>
            <p><strong>Example:</strong> AlphaGo mastering Go through self-play, receiving +1 for wins, -1 for losses</p>
            <p><strong>Advantage:</strong> Can discover superhuman strategies; learns sequential decision-making; doesn't need labeled data</p>
            <p><strong>Limitation:</strong> Requires many interactions (sample inefficient); reward design is critical and difficult; training is unstable</p>
            <p><strong>When to use:</strong> For sequential decision problems where you can simulate or interact with environment (games, robotics, resource management)</p>

            <p><strong>Comparison Summary:</strong></p>
            <p>Supervised learning is best when you have clear examples of correct behavior. Unsupervised learning excels at finding structure when you don't know what you're looking for. Reinforcement learning shines in interactive environments where optimal strategies emerge from trial and error. The choice depends on your data availability, problem structure, and whether you have labeled examples, need pattern discovery, or must learn sequential decision-making.</p>
        </div>

        <h2>ğŸ¯ Key Takeaways / é‡è¦ãƒã‚¤ãƒ³ãƒˆ</h2>

        <div class="highlight-box">
            <h3>Remember These Core Concepts:</h3>
            <ul>
                <li><strong>RL Components:</strong> Agent, Environment, State, Action, Reward, Policy</li>
                <li><strong>RL Cycle:</strong> Observe â†’ Act â†’ Reward â†’ Learn â†’ Repeat</li>
                <li><strong>Exploration-Exploitation:</strong> Balance trying new things vs. using what works (Îµ-greedy)</li>
                <li><strong>Q-Learning:</strong> Learn Q(s,a) values, choose action with max Q-value</li>
                <li><strong>Key Parameters:</strong> Î± (learning rate), Î³ (discount factor), Îµ (exploration rate)</li>
                <li><strong>DQN:</strong> Use neural networks for complex state spaces (Atari games)</li>
                <li><strong>AlphaGo:</strong> Supervised + RL defeated world champion</li>
                <li><strong>AlphaGo Zero:</strong> Pure self-play RL surpassed human knowledge</li>
            </ul>
        </div>

        <div class="navigation">
            <a href="../../index.html">â† Course Home</a>
            <a href="../week-12/lecture.html">â† Previous Week</a>
            <a href="slides.html">View Slides</a>
            <a href="assignment.html">Assignment</a>
            <a href="../week-14/lecture.html">Next Week: Exam Study Guide â†’</a>
        </div>

        <footer style="margin-top: 50px; padding-top: 20px; border-top: 2px solid #e5e7eb; text-align: center; color: #666;">
            <p>Week 13 Lecture Notes - Introduction to AI and Data Science</p>
            <p>Chukyo University - 2025</p>
        </footer>
    </div>
</body>
</html>